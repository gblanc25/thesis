{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "import osmnx as ox\n",
    "from pyrosm import OSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESS FLOW\n",
    "\n",
    "1. Download census tract data from https://www2.census.gov/geo/tiger/TIGER2024/BG/; note id from https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/\n",
    "2. Download OSM data for a given state here https://download.geofabrik.de/north-america/us.html\n",
    "3. Download ACS data from https://data.census.gov/table/ACSDT5Y2022.B19013; filter to \"every block group in {county of target city}; save as acs_city folder, rename nothing inside\n",
    "4. Run the following code block to process data\n",
    "5. Add new city to list of cities in graph code, run graph code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_ids = {\n",
    "    \"newyork\": 36,\n",
    "    \"losangeles\": \"06\",\n",
    "    \"chicago\": 17,\n",
    "    \"houston\": 48,\n",
    "    \"phoenix\": \"04\",\n",
    "    \"philadelphia\": 42,\n",
    "    \"sanantonio\": 48,\n",
    "    \"sandiego\": \"06\",\n",
    "    \"dallas\": 48,\n",
    "    \"jacksonville\": 12,\n",
    "    \"austin\": 48,\n",
    "    \"fortworth\": 48, \n",
    "    \"sanjose\": \"06\",\n",
    "    \"columbus\": 39,\n",
    "    \"charlotte\": 37,\n",
    "    \"indianapolis\": 18,\n",
    "    \"sanfrancisco\": \"06\",\n",
    "    \"seattle\": 53,\n",
    "    \"denver\": \"08\",\n",
    "    \"oklahomacity\": 40,\n",
    "    \"nashville\": 47,\n",
    "    \"washington\": 11,\n",
    "    \"elpaso\": 48,\n",
    "    \"lasvegas\": 32,\n",
    "    \"boston\": 25,\n",
    "    \"detroit\": 26,\n",
    "    \"portland\": 41,\n",
    "    \"louisville\": 21,\n",
    "    \"memphis\": 47,\n",
    "    \"baltimore\": 24,\n",
    "    \"milwaukee\": 55,\n",
    "    \"albuquerque\": 35,\n",
    "    \"tucson\": \"04\",\n",
    "    \"fresno\": \"06\",\n",
    "    \"sacramento\": \"06\",\n",
    "    \"mesa\": \"04\", \n",
    "    \"atlanta\": 13,\n",
    "    \"kansascity\": 20\n",
    "}\n",
    "\n",
    "cities_to_states = {\n",
    "    \"newyork\": \"new-york\",\n",
    "    \"losangeles\": \"california\",\n",
    "    \"chicago\": \"illinois\",\n",
    "    \"houston\": \"texas\",\n",
    "    \"phoenix\": \"arizona\",\n",
    "    \"philadelphia\": \"pennsylvania\",\n",
    "    \"sanantonio\": \"texas\",\n",
    "    \"sandiego\": \"california\",\n",
    "    \"dallas\": \"texas\",\n",
    "    \"jacksonville\": \"florida\",\n",
    "    \"austin\": \"texas\",\n",
    "    \"fortworth\": \"texas\", \n",
    "    \"sanjose\": \"california\",\n",
    "    \"columbus\": \"ohio\",\n",
    "    \"charlotte\": \"north-carolina\",\n",
    "    \"indianapolis\": \"indiana\",\n",
    "    \"sanfrancisco\": \"california\",\n",
    "    \"seattle\": \"washington\",\n",
    "    \"denver\": \"colorado\",\n",
    "    \"oklahomacity\": \"oklahoma\",\n",
    "    \"nashville\": \"tennessee\",\n",
    "    \"washington\": \"district-of-columbia\",\n",
    "    \"elpaso\": \"texas\",\n",
    "    \"lasvegas\": \"nevada\",\n",
    "    \"boston\": \"massachusetts\",\n",
    "    \"detroit\": \"michigan\",\n",
    "    \"portland\": \"oregon\",\n",
    "    \"louisville\": \"kentucky\",\n",
    "    \"memphis\": \"tennessee\",\n",
    "    \"baltimore\": \"maryland\",\n",
    "    \"milwaukee\": \"wisconsin\",\n",
    "    \"albuquerque\": \"new-mexico\",\n",
    "    \"tucson\": \"arizona\",\n",
    "    \"fresno\": \"california\",\n",
    "    \"sacramento\": \"california\",\n",
    "    \"mesa\": \"arizona\", \n",
    "    \"atlanta\": \"georgia\",\n",
    "    \"kansascity\": \"kansas\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_geojson(city, formal_city):\n",
    "#     # Download the Massachusetts OSM data (make sure to download the .osm.pbf file first)\n",
    "#     city_fp = f'{city}-latest.osm.pbf'\n",
    "\n",
    "#     # Define the boundaries of Boston (this will automatically query OSM for Boston's polygon)\n",
    "#     city_boundary = ox.geocode_to_gdf(f\"{formal_city}, USA\")\n",
    "\n",
    "#     print(\"boundary defined\")\n",
    "\n",
    "#     # Extract all building data within Boston\n",
    "#     tags = {\"amenity\": [\"library\", \"fire_station\", \"fast_food\", \"bank\", \"place_of_worship\", \"pharmacy\", \"social_facility\", \"police\", \"community_centre\"],\n",
    "#         \"leisure\": [\"park\"],\n",
    "#         \"building\": [\"school\", \"hospital\", \"residential\", \"house\", \"apartments\"],\n",
    "#            \"shop\": [\"supermarket\"],\n",
    "#            \"railway\": [\"subway\"]}  # Adjust the tags for other data types\n",
    "#     gdf = ox.features_from_polygon(city_boundary.geometry[0], tags)\n",
    "\n",
    "#     print(\"gdf extracted\")\n",
    "\n",
    "#     # Save the data to a file (e.g., GeoJSON)\n",
    "#     gdf.to_file(f\"buildings/{city}_buildings.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convert_to_geojson(city, formal_city):\n",
    "#     # Define the path to the local OSM file\n",
    "#     city_fp = f\"osm/{cities_to_states[city]}-latest.osm.pbf\"\n",
    "\n",
    "#     # Load the OSM data locally\n",
    "#     osm = OSM(city_fp)\n",
    "\n",
    "#     # Define the boundaries of the city (still using OSMnx to geocode)\n",
    "#     city_boundary = ox.geocode_to_gdf(f\"{formal_city}, USA\")\n",
    "#     print(\"Boundary defined\")\n",
    "\n",
    "#     # Get the boundary geometry\n",
    "#     city_geom = city_boundary.geometry.iloc[0]\n",
    "\n",
    "#     # Convert city geometry to bounding box (minx, miny, maxx, maxy)\n",
    "#     bbox = city_geom.bounds\n",
    "\n",
    "#     # Define the OSM tags you want to extract\n",
    "#     tags = {\n",
    "#         \"amenity\": [\"library\", \"fire_station\", \"fast_food\", \"bank\", \"place_of_worship\", \"pharmacy\", \"social_facility\", \"police\", \"community_centre\"],\n",
    "#         \"leisure\": [\"park\"],\n",
    "#         \"building\": [\"school\", \"hospital\", \"residential\", \"house\", \"apartments\"],\n",
    "#         \"landuse\": [\"residential\"],\n",
    "#         \"shop\": [\"supermarket\"],\n",
    "#         \"railway\": [\"subway\"]\n",
    "#     }\n",
    "\n",
    "#     # Extract data using pyrosm for each tag category\n",
    "#     gdfs = []\n",
    "#     for key, values in tags.items():\n",
    "#         gdf = osm.get_data_by_custom_criteria(custom_filter={key: values}, filter_type=\"keep\")\n",
    "        \n",
    "#         # Clip the data to the bounding box\n",
    "#         if gdf is not None and not gdf.empty:\n",
    "#             gdf = gdf[gdf.intersects(city_geom)]  # Filter by actual boundary\n",
    "#             gdfs.append(gdf)\n",
    "\n",
    "#     # Merge all extracted data into a single GeoDataFrame\n",
    "#     if gdfs:\n",
    "#         final_gdf = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True), crs=\"EPSG:4326\")\n",
    "#         print(\"Data extracted successfully\")\n",
    "\n",
    "#         # Save to a GeoJSON file\n",
    "#         output_fp = f\"buildings/{city}_buildings.geojson\"\n",
    "#         final_gdf.to_file(output_fp, driver=\"GeoJSON\")\n",
    "#         print(f\"GeoJSON saved at {output_fp}\")\n",
    "#     else:\n",
    "#         print(\"No data extracted for the specified tags.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_geojson(city, formal_city):\n",
    "    # Define the path to the local OSM file\n",
    "    city_fp = f\"osm/{cities_to_states[city]}-latest.osm.pbf\"\n",
    "\n",
    "    # Load the OSM data locally\n",
    "    osm = OSM(city_fp)\n",
    "\n",
    "    # Define the boundaries of the city (using OSMnx to geocode)\n",
    "    city_boundary = ox.geocode_to_gdf(f\"{formal_city}, USA\")\n",
    "    print(\"Boundary defined\")\n",
    "\n",
    "    # Get the city boundary geometry\n",
    "    city_geom = city_boundary.geometry.iloc[0]\n",
    "\n",
    "    # Define the OSM tags you want to extract (all in one dictionary)\n",
    "    tags = {\n",
    "        \"amenity\": [\"library\", \"fire_station\", \"bank\", \"place_of_worship\", \"pharmacy\", \"social_facility\", \"police\", \"community_centre\"],\n",
    "        \"leisure\": [\"park\"],\n",
    "        \"building\": [\"school\", \"hospital\", \"residential\", \"house\", \"apartments\"],\n",
    "        \"landuse\": [\"residential\"],\n",
    "        \"shop\": [\"supermarket\"],\n",
    "        \"highway\": [\"bus_stop\"]\n",
    "    }\n",
    "\n",
    "    # Perform a **single** call to get all features that match the tags\n",
    "    gdf = osm.get_data_by_custom_criteria(custom_filter=tags, filter_type=\"keep\")\n",
    "\n",
    "    # Filter data to keep only those within the city boundary\n",
    "    if gdf is not None and not gdf.empty:\n",
    "        try:\n",
    "            gdf = gdf[gdf.geometry.is_valid]\n",
    "            gdf = gdf[gdf.intersects(city_geom)]\n",
    "            gdf = gdf.drop(columns=[\"id\", \"timestamp\"], errors=\"ignore\")\n",
    "            print(\"Data extracted successfully\")\n",
    "        except:\n",
    "            invalid = gdf.loc[~gdf.geometry.is_valid]\n",
    "            print(invalid)\n",
    "\n",
    "        # Save to a GeoJSON file\n",
    "        output_fp = f\"buildings/{city}_buildings.geojson\"\n",
    "        gdf.to_file(output_fp, driver=\"GeoJSON\")\n",
    "        print(f\"GeoJSON saved at {output_fp}\")\n",
    "    else:\n",
    "        print(\"No data extracted for the specified tags.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 μs, sys: 1 μs, total: 3 μs\n",
      "Wall time: 4.77 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# GENERALIZABLE FUNCTION\n",
    "\n",
    "def process_data(city, id):\n",
    "\n",
    "    # Load the census tract shapefile\n",
    "    census_tracts = gpd.read_file(f'tl/tl_2024_{id}_bg/tl_2024_{id}_bg.shp')\n",
    "    \n",
    "    # Load the OSM data (make sure it includes location data as points or polygons)\n",
    "    osm_data = gpd.read_file(f'buildings/{city}_buildings.geojson')\n",
    "        \n",
    "\n",
    "    # Reproject if needed (make sure both are in the same CRS)\n",
    "    osm_data = osm_data.to_crs(census_tracts.crs)\n",
    "    \n",
    "    # Perform the spatial join\n",
    "    osm_with_geoid = gpd.sjoin(osm_data, census_tracts[['GEOID', 'geometry']], how='left')\n",
    "    \n",
    "    # Load the ACS data\n",
    "    acs_data = pd.read_csv(f\"acs/acs_{city}/ACSDT5Y2022.B19013-Data.csv\")\n",
    "    \n",
    "    acs_data = acs_data.rename(columns={\"B19013_001E\": \"MedHouseIncome\"})\n",
    "    acs_data[\"GEO_ID\"] = acs_data[\"GEO_ID\"].apply(lambda x: x[9:])\n",
    "    acs_data[\"MedHouseIncome\"] = acs_data[\"MedHouseIncome\"].apply(lambda x: float('nan') if x == \"-\" else x)\n",
    "    acs_data = acs_data.drop(0)\n",
    "\n",
    "    # Filter to include only BG_ID_10 and MedHouseIncome columns\n",
    "    acs_income = acs_data[['GEO_ID', 'MedHouseIncome']]\n",
    "    \n",
    "    acs_income = acs_income.astype(str)\n",
    "    acs_income = acs_income.dropna(subset=[\"MedHouseIncome\"])\n",
    "    \n",
    "    # Now merge the data\n",
    "    combined_with_income = osm_with_geoid.merge(acs_income, left_on=\"GEOID\", right_on=\"GEO_ID\", how=\"left\")\n",
    "\n",
    "    combined_with_income = combined_with_income.dropna(subset=[\"MedHouseIncome\"])\n",
    "        \n",
    "    return combined_with_income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6 μs, sys: 1 μs, total: 7 μs\n",
      "Wall time: 8.58 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cities = cities_to_ids.keys()\n",
    "\n",
    "formal_cities = [\"New York, New York\", \"Los Angeles, California\", \"Chicago, Illinois\", \"Houston, Texas\", \"Phoenix, Arizona\", \"Philadelphia, Pennsylvania\", \"San Antonio, Texas\", \"San Diego, California\",\n",
    "                 \"Dallas, Texas\", \"Jacksonville, Florida\", \"Austin, Texas\", \"Fort Worth, Texas\", \"San Jose, California\", \"Columbus, Ohio\", \"Charlotte, North Carolina\", \"Indianapolis, Indiana\", \"San Francisco, California\",\n",
    "                 \"Seattle, Washington\", \"Denver, Colorado\", \"Oklahoma City, Oklahoma\", \"Nashville, Tennessee\", \"Washington, District of Columbia\", \"El Paso, Texas\", \"Las Vegas, Nevada\", \"Boston, Massachusetts\",\n",
    "                 \"Detroit, Michigan\", \"Portland, Oregon\", \"Louisville, Kentucky\", \"Memphis, Tennessee\", \"Baltimore, Maryland\", \"Milwaukee, Wisconsin\", \"Albuquerque, New Mexico\", \"Tucson, Arizona\", \"Fresno, California\",\n",
    "                 \"Sacramento, California\", \"Mesa, Arizona\", \"Atlanta, Georgia\", \"Kansas City, Kansas\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boundary defined\n",
      "Data extracted successfully\n",
      "GeoJSON saved at buildings/newyork_buildings.geojson\n",
      "newyork complete! length 91615\n",
      "Boundary defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/home11/gleblanc/.conda/envs/OOD_env/lib/python3.11/site-packages/pyrosm/user_defined.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gdf = prepare_geodataframe(\n",
      "/n/home11/gleblanc/.conda/envs/OOD_env/lib/python3.11/site-packages/pyrosm/user_defined.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gdf = prepare_geodataframe(\n",
      "/n/home11/gleblanc/.conda/envs/OOD_env/lib/python3.11/site-packages/pyrosm/user_defined.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gdf = prepare_geodataframe(\n",
      "/n/home11/gleblanc/.conda/envs/OOD_env/lib/python3.11/site-packages/pyrosm/user_defined.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gdf = prepare_geodataframe(\n",
      "/n/home11/gleblanc/.conda/envs/OOD_env/lib/python3.11/site-packages/pyrosm/user_defined.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gdf = prepare_geodataframe(\n",
      "/n/home11/gleblanc/.conda/envs/OOD_env/lib/python3.11/site-packages/pyrosm/user_defined.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gdf = prepare_geodataframe(\n",
      "/n/home11/gleblanc/.conda/envs/OOD_env/lib/python3.11/site-packages/pyrosm/user_defined.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gdf = prepare_geodataframe(\n",
      "/n/home11/gleblanc/.conda/envs/OOD_env/lib/python3.11/site-packages/pyrosm/user_defined.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gdf = prepare_geodataframe(\n",
      "/n/home11/gleblanc/.conda/envs/OOD_env/lib/python3.11/site-packages/pyrosm/user_defined.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gdf = prepare_geodataframe(\n",
      "/n/home11/gleblanc/.conda/envs/OOD_env/lib/python3.11/site-packages/pyrosm/user_defined.py:58: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  gdf = prepare_geodataframe(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty GeoDataFrame\n",
      "Columns: [tags, lon, timestamp, changeset, visible, lat, version, id, addr:city, addr:country, addr:full, addr:housenumber, addr:housename, addr:postcode, addr:place, addr:street, email, name, opening_hours, operator, phone, ref, url, website, amenity, atm, bank, bicycle_parking, bar, building, building:levels, bus_stop, drinking_water, fast_food, ice_cream, internet_access, landuse, library, office, parking, post_office, restaurant, school, social_facility, source, start_date, wikipedia, leisure, outdoor_seating, building:flats, building:material, building:use, craft, height, shop, construction, railway, residential, alcohol, bicycle, boat, books, butcher, coffee, organic, religion, seafood, second_hand, disused, light_rail, tram, access, highway, lit, maxspeed, motorcar, surface, geometry, osm_type, charging_station, doctors, gambling, kindergarten, police, social_centre, spa, hackerspace, park, picnic_table, pitch, playground, swimming_pool, building:min_level, levels, basin, grass, industrial, meadow, military, clothes, ...]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 121 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 42\u001b[0m, in \u001b[0;36mconvert_to_geojson\u001b[0;34m(city, formal_city)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Save to a GeoJSON file\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     output_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbuildings/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_buildings.geojson\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 42\u001b[0m     \u001b[43mgdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGeoJSON\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGeoJSON saved at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_fp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/OOD_env/lib/python3.11/site-packages/geopandas/geodataframe.py:1249\u001b[0m, in \u001b[0;36mGeoDataFrame.to_file\u001b[0;34m(self, filename, driver, schema, index, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Write the ``GeoDataFrame`` to a file.\u001b[39;00m\n\u001b[1;32m   1159\u001b[0m \n\u001b[1;32m   1160\u001b[0m \u001b[38;5;124;03mBy default, an ESRI shapefile is written, but any OGR data source\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \n\u001b[1;32m   1246\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _to_file\n\u001b[0;32m-> 1249\u001b[0m \u001b[43m_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/OOD_env/lib/python3.11/site-packages/geopandas/io/file.py:610\u001b[0m, in \u001b[0;36m_to_file\u001b[0;34m(df, filename, driver, schema, index, mode, crs, engine, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m should be one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiona\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 610\u001b[0m     \u001b[43m_to_file_fiona\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyogrio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    612\u001b[0m     _to_file_pyogrio(df, filename, driver, schema, crs, mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/OOD_env/lib/python3.11/site-packages/geopandas/io/file.py:641\u001b[0m, in \u001b[0;36m_to_file_fiona\u001b[0;34m(df, filename, driver, schema, crs, mode, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m     crs_wkt \u001b[38;5;241m=\u001b[39m crs\u001b[38;5;241m.\u001b[39mto_wkt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWKT1_GDAL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m fiona\u001b[38;5;241m.\u001b[39mopen(\n\u001b[1;32m    639\u001b[0m     filename, mode\u001b[38;5;241m=\u001b[39mmode, driver\u001b[38;5;241m=\u001b[39mdriver, crs_wkt\u001b[38;5;241m=\u001b[39mcrs_wkt, schema\u001b[38;5;241m=\u001b[39mschema, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    640\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m colxn:\n\u001b[0;32m--> 641\u001b[0m     \u001b[43mcolxn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterecords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/OOD_env/lib/python3.11/site-packages/fiona/collection.py:541\u001b[0m, in \u001b[0;36mCollection.writerecords\u001b[0;34m(self, records)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    540\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollection not open for writing\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 541\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriterecs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession\u001b[38;5;241m.\u001b[39mget_length()\n\u001b[1;32m    543\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32mfiona/ogrext.pyx:1649\u001b[0m, in \u001b[0;36mfiona.ogrext.WritingSession.writerecs\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.conda/envs/OOD_env/lib/python3.11/site-packages/geopandas/geodataframe.py:932\u001b[0m, in \u001b[0;36mGeoDataFrame.iterfeatures\u001b[0;34m(self, na, show_bbox, drop_id)\u001b[0m\n\u001b[1;32m    929\u001b[0m na_mask \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39misna(properties_cols)\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnull\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 932\u001b[0m     \u001b[43mproperties\u001b[49m\u001b[43m[\u001b[49m\u001b[43mna_mask\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(properties\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[1;32m    935\u001b[0m     geom \u001b[38;5;241m=\u001b[39m geometries[i]\n",
      "File \u001b[0;32m~/.conda/envs/OOD_env/lib/python3.11/site-packages/pandas/core/frame.py:4297\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_slice(slc, value)\n\u001b[1;32m   4296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, DataFrame) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m-> 4297\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setitem_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, (Series, np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mlist\u001b[39m, Index)):\n\u001b[1;32m   4299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array(key, value)\n",
      "File \u001b[0;32m~/.conda/envs/OOD_env/lib/python3.11/site-packages/pandas/core/frame.py:4418\u001b[0m, in \u001b[0;36mDataFrame._setitem_frame\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   4414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust pass DataFrame or 2-d ndarray with boolean values only\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4415\u001b[0m     )\n\u001b[1;32m   4417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_setitem_copy()\n\u001b[0;32m-> 4418\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_where\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/OOD_env/lib/python3.11/site-packages/pandas/core/generic.py:10754\u001b[0m, in \u001b[0;36mNDFrame._where\u001b[0;34m(self, cond, other, inplace, axis, level, warn)\u001b[0m\n\u001b[1;32m  10748\u001b[0m     align \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m  10750\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m  10751\u001b[0m     \u001b[38;5;66;03m# we may have different type blocks come out of putmask, so\u001b[39;00m\n\u001b[1;32m  10752\u001b[0m     \u001b[38;5;66;03m# reconstruct the block manager\u001b[39;00m\n\u001b[0;32m> 10754\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mputmask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m  10755\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m  10756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(result)\n",
      "File \u001b[0;32m~/.conda/envs/OOD_env/lib/python3.11/site-packages/pandas/core/internals/base.py:226\u001b[0m, in \u001b[0;36mDataManager.putmask\u001b[0;34m(self, mask, new, align, warn)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m warn:\n\u001b[1;32m    224\u001b[0m         already_warned\u001b[38;5;241m.\u001b[39mwarned_already \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_with_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mputmask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43malign_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43malready_warned\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malready_warned\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/OOD_env/lib/python3.11/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/.conda/envs/OOD_env/lib/python3.11/site-packages/pandas/core/internals/blocks.py:1492\u001b[0m, in \u001b[0;36mBlock.putmask\u001b[0;34m(self, mask, new, using_cow, already_warned)\u001b[0m\n\u001b[1;32m   1489\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_copy(using_cow, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1490\u001b[0m     values \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m-> 1492\u001b[0m     \u001b[43mputmask_without_repeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m]\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m LossySetitemError:\n",
      "File \u001b[0;32m~/.conda/envs/OOD_env/lib/python3.11/site-packages/pandas/core/array_algos/putmask.py:98\u001b[0m, in \u001b[0;36mputmask_without_repeat\u001b[0;34m(values, mask, new)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot assign mismatch length to masked array\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mputmask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataframes = {}\n",
    "\n",
    "for city, formal_city in zip(cities_to_ids.keys(), formal_cities):\n",
    "    if not os.path.exists(f\"buildings/{city}_buildings.geojson\"):\n",
    "        convert_to_geojson(city, formal_city)\n",
    "    dataframes[city] = process_data(city, cities_to_ids[city])\n",
    "    print(city, \"complete! length\", len(dataframes[city]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will be important to note - how many block groups have we calculated, vs how many exist?\n",
    "\n",
    "Can also compare current income bins to expected income bins to get an idea for accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataframes.pkl', 'wb') as handle:\n",
    "    pickle.dump(dataframes, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataframes.pkl', 'rb') as handle:\n",
    "    dataframes = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(dataframes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newyork\n",
      "INCOME BINS for METHODS\n",
      " [  9024.          66674.66666667 100807.66666667 249653.        ]\n",
      "losangeles\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'losangeles'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:94\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:10\u001b[0m, in \u001b[0;36mcategory_normalized_counts_by_income\u001b[0;34m(categories, city_names, num_quintiles)\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'losangeles'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Define the function to calculate normalized counts by income range for multiple cities and categories\n",
    "def category_normalized_counts_by_income(categories, city_names, num_quintiles=3):\n",
    "    all_results = {}\n",
    "\n",
    "    # Process each city dataframe\n",
    "    for city_name in city_names:\n",
    "        \n",
    "        print(city_name)\n",
    "        \n",
    "        df = dataframes[city_name]\n",
    "        \n",
    "        df.drop(['income_range'], axis=1, errors='ignore', inplace=True)\n",
    "        \n",
    "        # Ensure MedHouseIncome is numeric\n",
    "        df[\"MedHouseIncome\"] = pd.to_numeric(df[\"MedHouseIncome\"], errors=\"coerce\")\n",
    "        \n",
    "        # Drop rows where MedHouseIncome is NaN after conversion\n",
    "        df = df.dropna(subset=[\"MedHouseIncome\"])\n",
    "\n",
    "        # Step 1: Calculate income quintiles based on unique block groups\n",
    "        unique_bg = df.drop_duplicates(subset=[\"GEO_ID\"])[[\"GEO_ID\", \"MedHouseIncome\"]]\n",
    "    \n",
    "        unique_bg[\"income_range\"], income_bins = pd.qcut(\n",
    "            unique_bg[\"MedHouseIncome\"], \n",
    "            q=num_quintiles, \n",
    "            retbins=True, \n",
    "            labels=[f\"Q{i+1}\" for i in range(num_quintiles)]\n",
    "        )\n",
    "    \n",
    "        \n",
    "        print(\"INCOME BINS for METHODS\\n\", income_bins)\n",
    "        \n",
    "        \n",
    "        # Step 2: Count the number of block groups in each income range\n",
    "        block_group_counts = unique_bg.groupby(\"income_range\", observed=False).size().reset_index(name=\"bg_count\") \n",
    "        \n",
    "        #print(\"BLOCK GROUP COUNTS\\n\", block_group_counts)\n",
    "\n",
    "        # Step 3: Merge income range back to the main DataFrame\n",
    "        df = pd.merge(df, unique_bg[[\"GEO_ID\", \"income_range\"]], on=\"GEO_ID\", how=\"left\")\n",
    "        city_results = {}\n",
    "        for category, items in categories.items():\n",
    "            category_results = {}\n",
    "            for item in items:\n",
    "                # Step 4: Filter for rows where the feature matches the specified item in the category\n",
    "                filtered_data = df[df[category] == item]\n",
    "                \n",
    "                #print(filtered_data.head())\n",
    "                \n",
    "                # Step 5: Count the occurrences of the item per income range\n",
    "                counts_by_income = filtered_data.groupby(\"income_range\", observed=False).size().reset_index(name=\"count\")\n",
    "                # Step 6: Merge with block group counts to get `bg_count` for each income range\n",
    "                counts_with_bg = pd.merge(counts_by_income, block_group_counts, on=\"income_range\", how=\"left\")\n",
    "                # Step 7: Calculate the normalized count (service count per block group) across cities\n",
    "                counts_with_bg[\"normalized_count\"] = counts_with_bg[\"count\"] / counts_with_bg[\"bg_count\"]\n",
    "\n",
    "                # Store the results for each item in the current category\n",
    "                category_results[item] = counts_with_bg[[\"income_range\", \"normalized_count\"]]\n",
    "            \n",
    "            # Store the category results in the main results dictionary for this city\n",
    "            city_results[category] = category_results\n",
    "        \n",
    "        # Store all results for each city by name\n",
    "        all_results[city_name] = city_results\n",
    "        dataframes[city_name] = df\n",
    "    \n",
    "    return all_results, income_bins\n",
    "\n",
    "\n",
    "# Function to calculate concavity using a quadratic fit\n",
    "def calculate_concavity(data):\n",
    "    # Replace NaNs with 0 for concavity calculation if necessary\n",
    "    y = data[\"normalized_count\"].fillna(0).values\n",
    "    x = np.arange(len(y))\n",
    "    \n",
    "    # Fit a quadratic curve (second-degree polynomial) to the data\n",
    "    coeffs = np.polyfit(x, y, 2)\n",
    "    \n",
    "    # The concavity is the coefficient of the x^2 term\n",
    "    concavity = coeffs[0]\n",
    "    return concavity\n",
    "\n",
    "# Define the categories and their corresponding items\n",
    "categories = {\n",
    "    \"amenity\": [\"library\", \"fire_station\", \"fast_food\", \"bank\", \"place_of_worship\", \"pharmacy\", \"social_facility\", \"police\", \"community_centre\"],\n",
    "    \"leisure\": [\"park\"],\n",
    "    \"building\": [\"school\", \"hospital\"]\n",
    "}\n",
    "\n",
    "# List of city dataframes and corresponding names\n",
    "city_names = list(cities_to_ids.keys())\n",
    "\n",
    "# Run the function to get results and income bins\n",
    "results, income_bins = category_normalized_counts_by_income(categories, city_names)\n",
    "\n",
    "# Determine grid dimensions based on total items across all categories\n",
    "total_items = sum(len(items) for items in categories.values())\n",
    "cols = 3  # Number of columns in the grid\n",
    "rows = math.ceil(total_items / cols)  # Calculate the number of rows required\n",
    "\n",
    "# Plotting in a 2D grid\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(15, rows * 5))\n",
    "#fig.suptitle(\"Normalized Amenity to Block Group Ratio by Income Tertile\")\n",
    "\n",
    "# Track the subplot index\n",
    "plot_index = 0\n",
    "\n",
    "# Loop through each category and item\n",
    "for category, items in categories.items():\n",
    "    for item in items:\n",
    "        # Determine the current row and column based on plot_index\n",
    "        row = plot_index // cols\n",
    "        col = plot_index % cols\n",
    "        ax = axs[row, col] if rows > 1 else axs[col]  # Handle single-row or multi-row cases\n",
    "\n",
    "        # Plot each city's data for the current item and calculate concavity\n",
    "        for idx, (city_name, city_data) in enumerate(results.items()):\n",
    "            data = city_data[category][item]\n",
    "            ax.plot(data[\"income_range\"], data[\"normalized_count\"], marker='o', linestyle='-', label=f\"{city_name}\")\n",
    "            \n",
    "#             # Calculate concavity\n",
    "#             concavity = calculate_concavity(data)\n",
    "            \n",
    "#             # Position the label vertically based on the city index to avoid overlap\n",
    "#             y_position = 0.9 - (idx * 0.1)  # Stagger vertically by 0.1 per city\n",
    "#             ax.text(0.05, y_position, f\"{city_name} Concavity: {concavity:.5f}\", transform=ax.transAxes, fontsize=10, verticalalignment='top')\n",
    "\n",
    "        # Set titles and labels for each subplot\n",
    "        ax.set_title(f\"{category.capitalize()}: {item.capitalize()}\")\n",
    "        ax.set_xlabel(\"Income Tertile\")\n",
    "        ax.set_ylabel(\"Amenity Count : Block Group Ratio\")\n",
    "        ax.legend()\n",
    "        \n",
    "        plot_index += 1\n",
    "\n",
    "# Hide any unused subplots if the grid has extra cells\n",
    "for j in range(plot_index, rows * cols):\n",
    "    fig.delaxes(axs[j // cols, j % cols])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit the main title\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mresults\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = {\n",
    "    \"quality-of-life\": {\n",
    "        \"amenity\": [\"library\", \"place_of_worship\", \"fast_food\"],\n",
    "        \"building\": [],\n",
    "        \"leisure\": [\"park\"]\n",
    "    },\n",
    "    \"economic-mobility\": {\n",
    "        \"amenity\": [\"social_facility\", \"bank\", \"community_centre\"],\n",
    "        \"building\": [],\n",
    "        \"leisure\": [\"school\"]\n",
    "    },\n",
    "    \"health-and-safety\": {\n",
    "        \"amenity\": [\"fire_station\", \"police\", \"pharmacy\"],\n",
    "        \"building\": [\"hospital\"],\n",
    "        \"leisure\": []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_names = cities_to_ids.keys()\n",
    "\n",
    "categories = {\n",
    "    \"amenity\": [\"library\", \"fire_station\", \"fast_food\", \"bank\", \"place_of_worship\", \"pharmacy\", \"social_facility\", \"police\", \"community_centre\"],\n",
    "    \"leisure\": [\"park\"],\n",
    "    \"building\": [\"school\", \"hospital\"]\n",
    "}\n",
    "\n",
    "class_counts = {}\n",
    "\n",
    "for city in city_names:\n",
    "    class_counts[city] = {\n",
    "        \"CCU\": {\n",
    "            \"quality-of-life\": 0,\n",
    "            \"economic-mobility\": 0,\n",
    "            \"health-and-safety\": 0,\n",
    "        },\n",
    "        \"CCD\": {\n",
    "            \"quality-of-life\": 0,\n",
    "            \"economic-mobility\": 0,\n",
    "            \"health-and-safety\": 0,\n",
    "        },\n",
    "        \"LP\": {\n",
    "            \"quality-of-life\": 0,\n",
    "            \"economic-mobility\": 0,\n",
    "            \"health-and-safety\": 0,\n",
    "        },\n",
    "        \"LN\": {\n",
    "            \"quality-of-life\": 0,\n",
    "            \"economic-mobility\": 0,\n",
    "            \"health-and-safety\": 0,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for category, items in categories.items():\n",
    "        try:\n",
    "            for item in items:\n",
    "                distances = list(results[city][category][item][\"normalized_count\"])\n",
    "                q1 = distances[0]\n",
    "                q2 = distances[1]\n",
    "                q3 = distances[2]\n",
    "\n",
    "                if item in classifications[\"quality-of-life\"][category]:\n",
    "                    purpose = \"quality-of-life\"\n",
    "                elif item in classifications[\"economic-mobility\"][category]:\n",
    "                    purpose = \"economic-mobility\"\n",
    "                else:\n",
    "                    purpose = \"health-and-safety\"\n",
    "\n",
    "                if q1 > q2 < q3:\n",
    "                    class_counts[city][\"CCU\"][purpose] += 1\n",
    "                elif q1 < q2 > q3:\n",
    "                    class_counts[city][\"CCD\"][purpose] += 1\n",
    "                elif q1 < q2 < q3:\n",
    "                    class_counts[city][\"LP\"][purpose] += 1\n",
    "                elif q1 > q2 > q3:\n",
    "                    class_counts[city][\"LN\"][purpose] += 1\n",
    "        except KeyError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mux = pd.MultiIndex.from_product([city_names, [\"CCU\", \"CCD\", \"LP\", \"LN\"], [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]])\n",
    "df = pd.DataFrame(class_counts, columns=mux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the nested dictionary into a DataFrame-friendly format\n",
    "flattened_data = []\n",
    "\n",
    "for city, categories in class_counts.items():\n",
    "    for category, metrics in categories.items():\n",
    "        for metric, value in metrics.items():\n",
    "            flattened_data.append((city, category, metric, value))\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = pd.DataFrame(flattened_data, columns=[\"City\", \"Category\", \"Metric\", \"Value\"])\n",
    "\n",
    "# Pivot to create MultiIndex columns\n",
    "df_pivot = df.pivot(index=\"City\", columns=[\"Metric\", \"Category\"], values=\"Value\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten and sum the nested dictionary\n",
    "category_counts = {\"CCD\": {}, \"CCU\": {}, \"LP\": {}, \"LN\": {}}\n",
    "\n",
    "for city, categories in class_counts.items():\n",
    "    \n",
    "    city_totals = {}\n",
    "    for category, metrics in categories.items():\n",
    "        # Sum across all metrics for each category\n",
    "        category_counts[category][city] = sum(metrics.values())\n",
    "\n",
    "# Convert to DataFrame\n",
    "counts_df = pd.DataFrame(category_counts)\n",
    "\n",
    "# Display the DataFrame\n",
    "counts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the values grouped by Metric and Category\n",
    "df_avg = df.groupby([\"Metric\", \"Category\"])[\"Value\"].mean().reset_index()\n",
    "\n",
    "# Unique metrics for separate subplots\n",
    "metrics = df_avg[\"Metric\"].unique()\n",
    "\n",
    "# Create a grid of bar plots\n",
    "fig, axs = plt.subplots(1, len(metrics), figsize=(15, 6), sharey=True)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    metric_data = df_avg[df_avg[\"Metric\"] == metric]\n",
    "    axs[i].bar(metric_data[\"Category\"], metric_data[\"Value\"], color='skyblue')\n",
    "    axs[i].set_title(f\"{metric.capitalize()}\")\n",
    "    axs[i].set_xlabel(\"Category\")\n",
    "    axs[i].set_ylabel(\"Value\" if i == 0 else \"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_founding = {\n",
    "    \"newyork\": 1624,\n",
    "    \"losangeles\": 1781,\n",
    "    \"chicago\": 1837,\n",
    "    \"houston\": 1837,\n",
    "    \"phoenix\": 1881,\n",
    "    \"philadelphia\": 1682,\n",
    "    \"sanantonio\": 1718,\n",
    "    \"sandiego\": 1769,\n",
    "    \"dallas\": 1841,\n",
    "    \"jacksonville\": 1822,\n",
    "    \"austin\": 1839,\n",
    "    \"fortworth\": 1849, \n",
    "    \"sanjose\": 1777,\n",
    "    \"columbus\": 1812,\n",
    "    \"charlotte\": 1768,\n",
    "    \"indianapolis\": 1821,\n",
    "    \"sanfrancisco\": 1776,\n",
    "    \"seattle\": 1851,\n",
    "    \"denver\": 1858,\n",
    "    \"oklahomacity\": 1889,\n",
    "    \"nashville\": 1779,\n",
    "    \"washington\": 1790,\n",
    "    \"elpaso\": 1873,\n",
    "    \"lasvegas\": 1905,\n",
    "    \"boston\": 1630,\n",
    "    \"detroit\": 1701,\n",
    "    \"portland\": 1851,\n",
    "    \"louisville\": 1778,\n",
    "    \"memphis\": 1819,\n",
    "    \"baltimore\": 1729,\n",
    "    \"milwaukee\": 1846,\n",
    "    \"albuquerque\": 1706,\n",
    "    \"tucson\": 1775,\n",
    "    \"fresno\": 1872,\n",
    "    \"sacramento\": 1848,\n",
    "    \"mesa\": 1878, \n",
    "    \"atlanta\": 1837,\n",
    "    \"kansascity\": 1861\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add founding date as a new column\n",
    "df_pivot[\"Founding Date\"] = df_pivot.index.get_level_values(\"City\").map(cities_to_founding)\n",
    "\n",
    "# Sort by the founding date\n",
    "df_sorted = df_pivot.reset_index().sort_values(by=\"Founding Date\").set_index([\"City\"])\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Initialize subplots for the three amenities\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# List of amenities (metrics)\n",
    "amenities = [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]\n",
    "\n",
    "trendlines = [\"CCU\", \"CCD\", \"LN\", \"LP\"]\n",
    "\n",
    "# Colors for each category\n",
    "colors = {\"CCU\": \"blue\", \"CCD\": \"green\", \"LN\": \"orange\", \"LP\": \"purple\"}\n",
    "\n",
    "# Iterate over each amenity to create a subplot\n",
    "for i, amenity in enumerate(amenities):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    # Filter data for the current amenity\n",
    "    test_df = df_sorted.loc[:, df_sorted.columns.get_level_values('Metric').isin([amenity, 'Founding Date'])]\n",
    "    test_df = test_df.loc[:, test_df.columns.get_level_values(\"Category\").isin(trendlines + [''])]\n",
    "\n",
    "    # Plot trendlines for each category\n",
    "    for category in trendlines:\n",
    "        # Extract founding dates and values for the category\n",
    "        founding_dates = test_df[\"Founding Date\"].values\n",
    "        values = test_df[(amenity, category)].values.flatten()\n",
    "\n",
    "        # Perform linear regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(founding_dates, values)\n",
    "\n",
    "        # Generate trendline\n",
    "        trendline = slope * founding_dates + intercept\n",
    "        ax.plot(founding_dates, trendline, color=colors[category], linestyle='--', label=f\"{category} Trendline\")\n",
    "        \n",
    "        # Print statistical results\n",
    "        print(f\"{amenity} - {category}:\")\n",
    "        print(f\"  Slope: {slope:.3f}\")\n",
    "        print(f\"  Intercept: {intercept:.3f}\")\n",
    "        print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "        print(f\"  P-value: {p_value:.3e}\")\n",
    "        print(f\"  Std Err: {std_err:.3f}\")\n",
    "    \n",
    "    # Customize the subplot\n",
    "    ax.set_title(f\"{amenity.capitalize()}\")\n",
    "    ax.set_xlabel(\"Founding Date\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Count\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "add test for statistical significance :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_population = {\n",
    "    \"newyork\": 8419,\n",
    "    \"losangeles\": 3985,\n",
    "    \"chicago\": 2716,\n",
    "    \"houston\": 2320,\n",
    "    \"phoenix\": 1703,\n",
    "    \"philadelphia\": 1584,\n",
    "    \"sanantonio\": 1548,\n",
    "    \"sandiego\": 1424,\n",
    "    \"dallas\": 1358,\n",
    "    \"jacksonville\": 977,\n",
    "    \"austin\": 978,\n",
    "    \"fortworth\": 942,\n",
    "    \"sanjose\": 1021,\n",
    "    \"columbus\": 907,\n",
    "    \"charlotte\": 912,\n",
    "    \"indianapolis\": 887,\n",
    "    \"sanfrancisco\": 815,\n",
    "    \"seattle\": 773,\n",
    "    \"denver\": 739,\n",
    "    \"oklahomacity\": 701,\n",
    "    \"nashville\": 691,\n",
    "    \"washington\": 706,\n",
    "    \"elpaso\": 681,\n",
    "    \"lasvegas\": 675,\n",
    "    \"boston\": 654,\n",
    "    \"detroit\": 639,\n",
    "    \"portland\": 654,\n",
    "    \"louisville\": 627,\n",
    "    \"memphis\": 621,\n",
    "    \"baltimore\": 575,\n",
    "    \"milwaukee\": 569,\n",
    "    \"albuquerque\": 563,\n",
    "    \"tucson\": 544,\n",
    "    \"fresno\": 545,\n",
    "    \"sacramento\": 524,\n",
    "    \"mesa\": 526,\n",
    "    \"atlanta\": 515,\n",
    "    \"kansascity\": 508\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add founding date as a new column\n",
    "df_pivot[\"Population\"] = df_pivot.index.get_level_values(\"City\").map(cities_to_population)\n",
    "\n",
    "# Sort by the founding date\n",
    "df_sorted = df_pivot.reset_index().sort_values(by=\"Population\").set_index([\"City\"])\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Initialize subplots for the three amenities\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# List of amenities (metrics)\n",
    "amenities = [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]\n",
    "\n",
    "trendlines = [\"CCU\", \"CCD\", \"LN\", \"LP\"]\n",
    "\n",
    "# Colors for each category\n",
    "colors = {\"CCU\": \"blue\", \"CCD\": \"green\", \"LN\": \"orange\", \"LP\": \"purple\"}\n",
    "\n",
    "# Iterate over each amenity to create a subplot\n",
    "for i, amenity in enumerate(amenities):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    # Filter data for the current amenity\n",
    "    test_df = df_sorted.loc[:, df_sorted.columns.get_level_values('Metric').isin([amenity, 'Population'])]\n",
    "    test_df = test_df.loc[:, test_df.columns.get_level_values(\"Category\").isin(trendlines + [''])]\n",
    "\n",
    "    # Plot trendlines for each category\n",
    "    for category in trendlines:\n",
    "        # Extract population and values for the category\n",
    "        populations = test_df[\"Population\"].values\n",
    "        values = test_df[(amenity, category)].values.flatten()\n",
    "\n",
    "        # Perform linear regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(populations, values)\n",
    "\n",
    "        # Generate trendline\n",
    "        trendline = slope * populations + intercept\n",
    "        ax.plot(populations, trendline, color=colors[category], linestyle='--', label=f\"{category} Trendline\")\n",
    "        \n",
    "        # Print statistical results\n",
    "        print(f\"{amenity} - {category}:\")\n",
    "        print(f\"  Slope: {slope:.3f}\")\n",
    "        print(f\"  Intercept: {intercept:.3f}\")\n",
    "        print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "        print(f\"  P-value: {p_value:.3e}\")\n",
    "        print(f\"  Std Err: {std_err:.3f}\")\n",
    "    \n",
    "    # Customize the subplot\n",
    "    ax.set_title(f\"{amenity.capitalize()}\")\n",
    "    ax.set_xlabel(\"Population (Thousands)\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Count\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cities_to_region = {\n",
    "#     \"newyork\": \"Northeast\",\n",
    "#     \"losangeles\": \"West\",\n",
    "#     \"chicago\": \"Midwest\",\n",
    "#     \"houston\": \"Southwest\",\n",
    "#     \"phoenix\": \"Southwest\",\n",
    "#     \"philadelphia\": \"Northeast\",\n",
    "#     \"sanantonio\": \"Southwest\",\n",
    "#     \"sandiego\": \"West\",\n",
    "#     \"dallas\": \"Southwest\",\n",
    "#     \"jacksonville\": \"Southeast\",\n",
    "#     \"austin\": \"Southwest\",\n",
    "#     \"fortworth\": \"Southwest\",\n",
    "#     \"sanjose\": \"West\",\n",
    "#     \"columbus\": \"Midwest\",\n",
    "#     \"charlotte\": \"Southeast\",\n",
    "#     \"indianapolis\": \"Midwest\",\n",
    "#     \"sanfrancisco\": \"West\",\n",
    "#     \"seattle\": \"West\",\n",
    "#     \"denver\": \"West\",\n",
    "#     \"oklahomacity\": \"Southwest\",\n",
    "#     \"nashville\": \"Southeast\",\n",
    "#     \"washington\": \"Northeast\",\n",
    "#     \"elpaso\": \"Southwest\",\n",
    "#     \"lasvegas\": \"West\",\n",
    "#     \"boston\": \"Northeast\",\n",
    "#     \"detroit\": \"Midwest\",\n",
    "#     \"portland\": \"West\",\n",
    "#     \"louisville\": \"Midwest\",\n",
    "#     \"memphis\": \"Southeast\",\n",
    "#     \"baltimore\": \"Northeast\",\n",
    "#     \"milwaukee\": \"Midwest\",\n",
    "#     \"albuquerque\": \"Southwest\",\n",
    "#     \"tucson\": \"Southwest\",\n",
    "#     \"fresno\": \"West\",\n",
    "#     \"sacramento\": \"West\",\n",
    "#     \"mesa\": \"Southwest\",\n",
    "#     \"atlanta\": \"Southeast\",\n",
    "#     \"kansascity\": \"Midwest\"\n",
    "# }\n",
    "\n",
    "# Approximate distances (in miles) from Los Angeles, CA\n",
    "cities_to_distance = {\n",
    "    \"newyork\": 2790,\n",
    "    \"losangeles\": 0,\n",
    "    \"chicago\": 2015,\n",
    "    \"houston\": 1540,\n",
    "    \"phoenix\": 370,\n",
    "    \"philadelphia\": 2720,\n",
    "    \"sanantonio\": 1350,\n",
    "    \"sandiego\": 120,\n",
    "    \"dallas\": 1435,\n",
    "    \"jacksonville\": 2410,\n",
    "    \"austin\": 1375,\n",
    "    \"fortworth\": 1410,\n",
    "    \"sanjose\": 340,\n",
    "    \"columbus\": 2240,\n",
    "    \"charlotte\": 2430,\n",
    "    \"indianapolis\": 2060,\n",
    "    \"sanfrancisco\": 380,\n",
    "    \"seattle\": 960,\n",
    "    \"denver\": 1020,\n",
    "    \"oklahomacity\": 1320,\n",
    "    \"nashville\": 2100,\n",
    "    \"washington\": 2680,\n",
    "    \"elpaso\": 800,\n",
    "    \"lasvegas\": 270,\n",
    "    \"boston\": 2990,\n",
    "    \"detroit\": 2280,\n",
    "    \"portland\": 970,\n",
    "    \"louisville\": 2120,\n",
    "    \"memphis\": 1815,\n",
    "    \"baltimore\": 2690,\n",
    "    \"milwaukee\": 2040,\n",
    "    \"albuquerque\": 790,\n",
    "    \"tucson\": 485,\n",
    "    \"fresno\": 230,\n",
    "    \"sacramento\": 380,\n",
    "    \"mesa\": 370,\n",
    "    \"atlanta\": 2180,\n",
    "    \"kansascity\": 1600,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add founding date as a new column\n",
    "df_pivot[\"Distance\"] = df_pivot.index.get_level_values(\"City\").map(cities_to_distance)\n",
    "\n",
    "# Sort by the founding date\n",
    "df_sorted = df_pivot.reset_index().sort_values(by=\"Distance\").set_index([\"City\"])\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Initialize subplots for the three amenities\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# List of amenities (metrics)\n",
    "amenities = [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]\n",
    "\n",
    "# Colors for each category\n",
    "colors = {\"CCU\": \"blue\", \"CCD\": \"green\", \"LN\": \"orange\", \"LP\": \"purple\"}\n",
    "\n",
    "# Iterate over each amenity to create a subplot\n",
    "for i, amenity in enumerate(amenities):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    # Filter data for the current amenity\n",
    "    test_df = df_sorted.loc[:, df_sorted.columns.get_level_values('Metric').isin([amenity, 'Distance'])]\n",
    "    test_df = test_df.loc[:, test_df.columns.get_level_values(\"Category\").isin(trendlines + [''])]\n",
    "    \n",
    "    # Plot trendlines for each category\n",
    "    for category in trendlines:\n",
    "        # Extract founding dates (Distance) and values for the category\n",
    "        founding_dates = test_df[\"Distance\"].values\n",
    "        values = test_df[(amenity, category)].values.flatten()\n",
    "\n",
    "        # Perform linear regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(founding_dates, values)\n",
    "        \n",
    "        # Generate trendline\n",
    "        trendline = slope * founding_dates + intercept\n",
    "        ax.plot(founding_dates, trendline, color=colors[category], linestyle='--', label=f\"{category} Trendline\")\n",
    "        \n",
    "        # Display statistical information in console\n",
    "        print(f\"{amenity} - {category}:\")\n",
    "        print(f\"  Slope: {slope:.3f}\")\n",
    "        print(f\"  Intercept: {intercept:.3f}\")\n",
    "        print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "        print(f\"  P-value: {p_value:.3e}\")\n",
    "        print(f\"  Std Err: {std_err:.3f}\")\n",
    "    \n",
    "    # Customize the subplot\n",
    "    ax.set_title(f\"{amenity.capitalize()}\")\n",
    "    ax.set_xlabel(\"Distance (Miles)\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Count\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Census table B19083\n",
    "cities_to_gini = {\n",
    "    \"newyork\": 0.515,\n",
    "    \"losangeles\": 0.4879,\n",
    "    \"chicago\": 0.4798,\n",
    "    \"houston\": 0.4809,\n",
    "    \"phoenix\": 0.4564,\n",
    "    \"philadelphia\": 0.4824,\n",
    "    \"sanantonio\": 0.4591,\n",
    "    \"sandiego\": 0.4587,\n",
    "    \"dallas\": 0.4664,\n",
    "    \"jacksonville\": 0.4746,\n",
    "    \"austin\": 0.4734,\n",
    "    \"fortworth\": 0.4664,\n",
    "    \"sanjose\": 0.4811,\n",
    "    \"columbus\": 0.4641,\n",
    "    \"charlotte\": 0.4752,\n",
    "    \"indianapolis\": 0.4572,\n",
    "    \"sanfrancisco\": 0.4985,\n",
    "    \"seattle\": 0.4688,\n",
    "    \"denver\": 0.4518,\n",
    "    \"oklahomacity\": 0.4734,\n",
    "    \"nashville\": 0.4624,\n",
    "    \"washington\": 0.4472,\n",
    "    \"elpaso\": 0.4652,\n",
    "    \"lasvegas\": 0.4657,\n",
    "    \"boston\": 0.4836,\n",
    "    \"detroit\": 0.4738,\n",
    "    \"portland\": 0.4490,\n",
    "    \"louisville\": 0.4651,\n",
    "    \"memphis\": 0.4756,\n",
    "    \"baltimore\": 0.4623,\n",
    "    \"milwaukee\": 0.4771,\n",
    "    \"albuquerque\": 0.46,\n",
    "    \"tucson\": 0.4667,\n",
    "    \"fresno\": 0.4695,\n",
    "    \"sacramento\": 0.4510,\n",
    "    \"mesa\": 0.4564,\n",
    "    \"atlanta\": 0.4679,\n",
    "    \"kansascity\": 0.4507,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add founding date as a new column\n",
    "df_pivot[\"Gini\"] = df_pivot.index.get_level_values(\"City\").map(cities_to_gini)\n",
    "\n",
    "# Sort by the founding date\n",
    "df_sorted = df_pivot.reset_index().sort_values(by=\"Distance\").set_index([\"City\"])\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Initialize subplots for the three amenities\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# List of amenities (metrics)\n",
    "amenities = [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]\n",
    "\n",
    "# Colors for each category\n",
    "colors = {\"CCU\": \"blue\", \"CCD\": \"green\", \"LN\": \"orange\", \"LP\": \"purple\"}\n",
    "\n",
    "# Iterate over each amenity to create a subplot\n",
    "for i, amenity in enumerate(amenities):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    # Filter data for the current amenity\n",
    "    test_df = df_sorted.loc[:, df_sorted.columns.get_level_values('Metric').isin([amenity, 'Gini'])]\n",
    "    test_df = test_df.loc[:, test_df.columns.get_level_values(\"Category\").isin(trendlines + [''])]\n",
    "    \n",
    "    # Plot trendlines for each category\n",
    "    for category in trendlines:\n",
    "        # Extract founding dates (Distance) and values for the category\n",
    "        founding_dates = test_df[\"Gini\"].values\n",
    "        values = test_df[(amenity, category)].values.flatten()\n",
    "\n",
    "        # Perform linear regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(founding_dates, values)\n",
    "        \n",
    "        # Generate trendline\n",
    "        trendline = slope * founding_dates + intercept\n",
    "        ax.plot(founding_dates, trendline, color=colors[category], linestyle='--', label=f\"{category} Trendline\")\n",
    "        \n",
    "        # Display statistical information in console\n",
    "        print(f\"{amenity} - {category}:\")\n",
    "        print(f\"  Slope: {slope:.3f}\")\n",
    "        print(f\"  Intercept: {intercept:.3f}\")\n",
    "        print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "        print(f\"  P-value: {p_value:.3e}\")\n",
    "        print(f\"  Std Err: {std_err:.3f}\")\n",
    "    \n",
    "    # Customize the subplot\n",
    "    ax.set_title(f\"{amenity.capitalize()}\")\n",
    "    ax.set_xlabel(\"Gini Index\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Count\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.wkt import loads as wkt_loads\n",
    "\n",
    "# Function to calculate minimum distances with sampling\n",
    "def calculate_min_distances_block_group(df, categories, block_group_column=\"GEO_ID\"):\n",
    "    projected_crs = \"EPSG:3857\"\n",
    "    results = {}\n",
    "    \n",
    "    # Convert WKT text to shapely geometry objects only if necessary\n",
    "    if df[\"geometry\"].dtype == \"object\":  # If geometry might contain mixed types\n",
    "        df[\"geometry\"] = df[\"geometry\"].apply(\n",
    "            lambda geom: wkt_loads(geom) if isinstance(geom, str) else geom\n",
    "        )\n",
    "\n",
    "    df = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "    # Reproject to the projected CRS for distance calculations\n",
    "    df = df.to_crs(projected_crs)\n",
    "\n",
    "    for category, items in categories.items():\n",
    "        category_results = {}\n",
    "        for item in items:\n",
    "            # Filter amenities by category and item\n",
    "            amenities = df[df[category] == item]\n",
    "            amenities = gpd.GeoDataFrame(amenities, geometry=\"geometry\").to_crs(projected_crs)\n",
    "            amenities[\"geometry\"] = amenities[\"geometry\"].apply(\n",
    "                lambda geom: geom.centroid if geom.geom_type == \"Polygon\" else geom\n",
    "            )\n",
    "            \n",
    "            # Filter residential buildings for distance calculation\n",
    "            residential_buildings = df[\n",
    "                (df[\"building\"].isin([\"residential\", \"house\", \"apartments\"])) |  # Residential buildings\n",
    "                (df[\"landuse\"] == \"residential\")  # Residential landuse\n",
    "            ].copy()\n",
    "            \n",
    "            # Randomly sample one residence per block group\n",
    "            sampled_residences = residential_buildings.groupby(block_group_column).sample(n=1, random_state=42)\n",
    "\n",
    "            # Compute minimum distance from sampled residences to each amenity\n",
    "            sampled_residences[f\"min_dist_to_{item}\"] = sampled_residences[\"geometry\"].apply(\n",
    "                lambda geom: amenities.distance(geom).min() if not amenities.empty else float(\"inf\")\n",
    "            )\n",
    "\n",
    "            # Calculate average distance per block group\n",
    "            block_group_avg = sampled_residences.groupby(block_group_column, observed=False).agg(\n",
    "                avg_dist=(f\"min_dist_to_{item}\", \"mean\"),\n",
    "                income_range=(\"income_range\", \"first\")\n",
    "            ).reset_index()\n",
    "\n",
    "            # Aggregate by income range\n",
    "            income_range_avg = block_group_avg.groupby(\"income_range\", observed=False).agg(\n",
    "                avg_dist=(\"avg_dist\", \"mean\")\n",
    "            ).reset_index()\n",
    "\n",
    "            # Store the results\n",
    "            category_results[item] = income_range_avg\n",
    "        results[category] = category_results\n",
    "    return results\n",
    "\n",
    "# Function to prepare data for graphing\n",
    "def prepare_distance_data(results_dict, categories):\n",
    "    results = {}\n",
    "    for category, items in categories.items():\n",
    "        category_results = {}\n",
    "        for item in items:\n",
    "            if item in results_dict[category]:\n",
    "                category_results[item] = results_dict[category][item]\n",
    "        results[category] = category_results\n",
    "    return results\n",
    "\n",
    "# Generalized plotting function\n",
    "def plot_distance_graphs(results_by_city, categories, city_names):\n",
    "    total_items = sum(len(items) for items in categories.values())\n",
    "    cols = 3\n",
    "    rows = math.ceil(total_items / cols)\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, rows * 5))\n",
    "    #fig.suptitle(\"Average Minimum Distance to Each Amenity by Income Quintile for Multiple Cities\")\n",
    "\n",
    "    plot_index = 0\n",
    "\n",
    "    for category, items in categories.items():\n",
    "        for item in items:\n",
    "            row = plot_index // cols\n",
    "            col = plot_index % cols\n",
    "            ax = axs[row, col] if rows > 1 else axs[col]\n",
    "\n",
    "            for city_name, city_results in results_by_city.items():\n",
    "                if item in city_results[category]:\n",
    "                    data = city_results[category][item]\n",
    "                    ax.plot(data[\"income_range\"], data.iloc[:, 1], marker='o', linestyle='-', label=city_name)\n",
    "\n",
    "            ax.set_title(f\"{category.capitalize()}: {item.capitalize()}\")\n",
    "            ax.set_xlabel(\"Income Tertile\")\n",
    "            ax.set_ylabel(\"Average Distance\")\n",
    "            ax.legend()\n",
    "            plot_index += 1\n",
    "\n",
    "    for j in range(plot_index, rows * cols):\n",
    "        fig.delaxes(axs[j // cols, j % cols])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Define categories\n",
    "categories = {\n",
    "    \"amenity\": [\"library\", \"fire_station\", \"fast_food\", \"bank\", \"place_of_worship\", \"pharmacy\", \"social_facility\", \"police\", \"community_centre\"],\n",
    "    \"leisure\": [\"park\"],\n",
    "    \"building\": [\"school\", \"hospital\"]\n",
    "}\n",
    "\n",
    "# List of city dataframes\n",
    "city_names = list(cities_to_ids.keys())\n",
    "\n",
    "results_by_city = {}\n",
    "for city_name in city_names:\n",
    "    df = dataframes[city_name]\n",
    "    results_by_city[city_name] = calculate_min_distances_block_group(df, categories, block_group_column=\"GEO_ID\")\n",
    "    print(city_name, \"done!\")\n",
    "\n",
    "# Prepare data for graphing\n",
    "prepared_results = {\n",
    "    city_name: prepare_distance_data(results, categories)\n",
    "    for city_name, results in results_by_city.items()\n",
    "}\n",
    "\n",
    "# Plot the results\n",
    "plot_distance_graphs(prepared_results, categories, city_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could also later on do the \"service walk index\"? But for now just looking at distance to a given type of entity. \n",
    "\n",
    "Do differences correspond to certain levels of income inequality?\n",
    "\n",
    "is there something with more residences stored?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalization based on apartment size? --> or number of units\n",
    "\n",
    "have to acknowledge that we're clearly missing some residences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: consider replacing fast food with something else.. like grocery store? idk\n",
    "\n",
    "can also do different kinds of social facilities..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = {\n",
    "    \"quality-of-life\": {\n",
    "        \"amenity\": [\"library\", \"place_of_worship\", \"fast_food\"],\n",
    "        \"building\": [],\n",
    "        \"leisure\": [\"park\"]\n",
    "    },\n",
    "    \"economic-mobility\": {\n",
    "        \"amenity\": [\"social_facility\", \"bank\", \"community_centre\"],\n",
    "        \"building\": [],\n",
    "        \"leisure\": [\"school\"]\n",
    "    },\n",
    "    \"health-and-safety\": {\n",
    "        \"amenity\": [\"fire_station\", \"police\", \"pharmacy\"],\n",
    "        \"building\": [\"hospital\"],\n",
    "        \"leisure\": []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_names = cities_to_ids.keys()\n",
    "\n",
    "categories = {\n",
    "    \"amenity\": [\"library\", \"fire_station\", \"fast_food\", \"bank\", \"place_of_worship\", \"pharmacy\", \"social_facility\", \"police\", \"community_centre\"],\n",
    "    \"leisure\": [\"park\"],\n",
    "    \"building\": [\"school\", \"hospital\"]\n",
    "}\n",
    "\n",
    "class_counts = {}\n",
    "\n",
    "for city in city_names:\n",
    "    class_counts[city] = {\n",
    "        \"CCU\": {\n",
    "            \"quality-of-life\": 0,\n",
    "            \"economic-mobility\": 0,\n",
    "            \"health-and-safety\": 0,\n",
    "        },\n",
    "        \"CCD\": {\n",
    "            \"quality-of-life\": 0,\n",
    "            \"economic-mobility\": 0,\n",
    "            \"health-and-safety\": 0,\n",
    "        },\n",
    "        \"LP\": {\n",
    "            \"quality-of-life\": 0,\n",
    "            \"economic-mobility\": 0,\n",
    "            \"health-and-safety\": 0,\n",
    "        },\n",
    "        \"LN\": {\n",
    "            \"quality-of-life\": 0,\n",
    "            \"economic-mobility\": 0,\n",
    "            \"health-and-safety\": 0,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for category, items in categories.items():\n",
    "        for item in items:\n",
    "            distances = list(prepared_results[city][category][item][\"avg_dist\"])\n",
    "            q1 = distances[0]\n",
    "            q2 = distances[1]\n",
    "            q3 = distances[2]\n",
    "            \n",
    "            if item in classifications[\"quality-of-life\"][category]:\n",
    "                purpose = \"quality-of-life\"\n",
    "            elif item in classifications[\"economic-mobility\"][category]:\n",
    "                purpose = \"economic-mobility\"\n",
    "            else:\n",
    "                purpose = \"health-and-safety\"\n",
    "            \n",
    "            if q1 > q2 < q3:\n",
    "                class_counts[city][\"CCU\"][purpose] += 1\n",
    "            elif q1 < q2 > q3:\n",
    "                class_counts[city][\"CCD\"][purpose] += 1\n",
    "            elif q1 < q2 < q3:\n",
    "                class_counts[city][\"LP\"][purpose] += 1\n",
    "            elif q1 > q2 > q3:\n",
    "                class_counts[city][\"LN\"][purpose] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mux = pd.MultiIndex.from_product([city_names, [\"CCU\", \"CCD\", \"LP\", \"LN\"], [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]])\n",
    "df = pd.DataFrame(class_counts, columns=mux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the nested dictionary into a DataFrame-friendly format\n",
    "flattened_data = []\n",
    "\n",
    "for city, categories in class_counts.items():\n",
    "    for category, metrics in categories.items():\n",
    "        for metric, value in metrics.items():\n",
    "            flattened_data.append((city, category, metric, value))\n",
    "\n",
    "# Convert to a DataFrame\n",
    "df = pd.DataFrame(flattened_data, columns=[\"City\", \"Category\", \"Metric\", \"Value\"])\n",
    "\n",
    "# Pivot to create MultiIndex columns\n",
    "df_pivot = df.pivot(index=\"City\", columns=[\"Metric\", \"Category\"], values=\"Value\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten and sum the nested dictionary\n",
    "category_counts = {\"CCD\": {}, \"CCU\": {}, \"LP\": {}, \"LN\": {}}\n",
    "\n",
    "for city, categories in class_counts.items():\n",
    "    \n",
    "    city_totals = {}\n",
    "    for category, metrics in categories.items():\n",
    "        # Sum across all metrics for each category\n",
    "        category_counts[category][city] = sum(metrics.values())\n",
    "\n",
    "# Convert to DataFrame\n",
    "counts_df = pd.DataFrame(category_counts)\n",
    "\n",
    "# Display the DataFrame\n",
    "counts_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average the values grouped by Metric and Category\n",
    "df_avg = df.groupby([\"Metric\", \"Category\"])[\"Value\"].mean().reset_index()\n",
    "\n",
    "# Unique metrics for separate subplots\n",
    "metrics = df_avg[\"Metric\"].unique()\n",
    "\n",
    "# Create a grid of bar plots\n",
    "fig, axs = plt.subplots(1, len(metrics), figsize=(15, 6), sharey=True)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    metric_data = df_avg[df_avg[\"Metric\"] == metric]\n",
    "    axs[i].bar(metric_data[\"Category\"], metric_data[\"Value\"], color='skyblue')\n",
    "    axs[i].set_title(f\"{metric.capitalize()}\")\n",
    "    axs[i].set_xlabel(\"Category\")\n",
    "    axs[i].set_ylabel(\"Value\" if i == 0 else \"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_founding = {\n",
    "    \"newyork\": 1624,\n",
    "    \"losangeles\": 1781,\n",
    "    \"chicago\": 1837,\n",
    "    \"houston\": 1837,\n",
    "    \"phoenix\": 1881,\n",
    "    \"philadelphia\": 1682,\n",
    "    \"sanantonio\": 1718,\n",
    "    \"sandiego\": 1769,\n",
    "    \"dallas\": 1841,\n",
    "    \"jacksonville\": 1822,\n",
    "    \"austin\": 1839,\n",
    "    \"fortworth\": 1849, \n",
    "    \"sanjose\": 1777,\n",
    "    \"columbus\": 1812,\n",
    "    \"charlotte\": 1768,\n",
    "    \"indianapolis\": 1821,\n",
    "    \"sanfrancisco\": 1776,\n",
    "    \"seattle\": 1851,\n",
    "    \"denver\": 1858,\n",
    "    \"oklahomacity\": 1889,\n",
    "    \"nashville\": 1779,\n",
    "    \"washington\": 1790,\n",
    "    \"elpaso\": 1873,\n",
    "    \"lasvegas\": 1905,\n",
    "    \"boston\": 1630,\n",
    "    \"detroit\": 1701,\n",
    "    \"portland\": 1851,\n",
    "    \"louisville\": 1778,\n",
    "    \"memphis\": 1819,\n",
    "    \"baltimore\": 1729,\n",
    "    \"milwaukee\": 1846,\n",
    "    \"albuquerque\": 1706,\n",
    "    \"tucson\": 1775,\n",
    "    \"fresno\": 1872,\n",
    "    \"sacramento\": 1848,\n",
    "    \"mesa\": 1878, \n",
    "    \"atlanta\": 1837,\n",
    "    \"kansascity\": 1861\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add founding date as a new column\n",
    "df_pivot[\"Founding Date\"] = df_pivot.index.get_level_values(\"City\").map(cities_to_founding)\n",
    "\n",
    "# Sort by the founding date\n",
    "df_sorted = df_pivot.reset_index().sort_values(by=\"Founding Date\").set_index([\"City\"])\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Initialize subplots for the three amenities\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# List of amenities (metrics)\n",
    "amenities = [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]\n",
    "\n",
    "trendlines = [\"CCU\", \"CCD\", \"LN\", \"LP\"]\n",
    "\n",
    "# Colors for each category\n",
    "colors = {\"CCU\": \"blue\", \"CCD\": \"green\", \"LN\": \"orange\", \"LP\": \"purple\"}\n",
    "\n",
    "# Iterate over each amenity to create a subplot\n",
    "for i, amenity in enumerate(amenities):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    # Filter data for the current amenity\n",
    "    test_df = df_sorted.loc[:, df_sorted.columns.get_level_values('Metric').isin([amenity, 'Founding Date'])]\n",
    "    test_df = test_df.loc[:, test_df.columns.get_level_values(\"Category\").isin(trendlines + [''])]\n",
    "\n",
    "    # Plot trendlines for each category\n",
    "    for category in trendlines:\n",
    "        # Extract founding dates and values for the category\n",
    "        founding_dates = test_df[\"Founding Date\"].values\n",
    "        values = test_df[(amenity, category)].values.flatten()\n",
    "\n",
    "        # Perform linear regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(founding_dates, values)\n",
    "\n",
    "        # Generate trendline\n",
    "        trendline = slope * founding_dates + intercept\n",
    "        ax.plot(founding_dates, trendline, color=colors[category], linestyle='--', label=f\"{category} Trendline\")\n",
    "        \n",
    "        # Print statistical results\n",
    "        print(f\"{amenity} - {category}:\")\n",
    "        print(f\"  Slope: {slope:.3f}\")\n",
    "        print(f\"  Intercept: {intercept:.3f}\")\n",
    "        print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "        print(f\"  P-value: {p_value:.3e}\")\n",
    "        print(f\"  Std Err: {std_err:.3f}\")\n",
    "    \n",
    "    # Customize the subplot\n",
    "    ax.set_title(f\"{amenity.capitalize()}\")\n",
    "    ax.set_xlabel(\"Founding Date\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Count\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_population = {\n",
    "    \"newyork\": 8419,\n",
    "    \"losangeles\": 3985,\n",
    "    \"chicago\": 2716,\n",
    "    \"houston\": 2320,\n",
    "    \"phoenix\": 1703,\n",
    "    \"philadelphia\": 1584,\n",
    "    \"sanantonio\": 1548,\n",
    "    \"sandiego\": 1424,\n",
    "    \"dallas\": 1358,\n",
    "    \"jacksonville\": 977,\n",
    "    \"austin\": 978,\n",
    "    \"fortworth\": 942,\n",
    "    \"sanjose\": 1021,\n",
    "    \"columbus\": 907,\n",
    "    \"charlotte\": 912,\n",
    "    \"indianapolis\": 887,\n",
    "    \"sanfrancisco\": 815,\n",
    "    \"seattle\": 773,\n",
    "    \"denver\": 739,\n",
    "    \"oklahomacity\": 701,\n",
    "    \"nashville\": 691,\n",
    "    \"washington\": 706,\n",
    "    \"elpaso\": 681,\n",
    "    \"lasvegas\": 675,\n",
    "    \"boston\": 654,\n",
    "    \"detroit\": 639,\n",
    "    \"portland\": 654,\n",
    "    \"louisville\": 627,\n",
    "    \"memphis\": 621,\n",
    "    \"baltimore\": 575,\n",
    "    \"milwaukee\": 569,\n",
    "    \"albuquerque\": 563,\n",
    "    \"tucson\": 544,\n",
    "    \"fresno\": 545,\n",
    "    \"sacramento\": 524,\n",
    "    \"mesa\": 526,\n",
    "    \"atlanta\": 515,\n",
    "    \"kansascity\": 508\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add founding date as a new column\n",
    "df_pivot[\"Population\"] = df_pivot.index.get_level_values(\"City\").map(cities_to_population)\n",
    "\n",
    "# Sort by the founding date\n",
    "df_sorted = df_pivot.reset_index().sort_values(by=\"Population\").set_index([\"City\"])\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Initialize subplots for the three amenities\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# List of amenities (metrics)\n",
    "amenities = [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]\n",
    "\n",
    "trendlines = [\"CCU\", \"CCD\", \"LN\", \"LP\"]\n",
    "\n",
    "# Colors for each category\n",
    "colors = {\"CCU\": \"blue\", \"CCD\": \"green\", \"LN\": \"orange\", \"LP\": \"purple\"}\n",
    "\n",
    "# Iterate over each amenity to create a subplot\n",
    "for i, amenity in enumerate(amenities):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    # Filter data for the current amenity\n",
    "    test_df = df_sorted.loc[:, df_sorted.columns.get_level_values('Metric').isin([amenity, 'Population'])]\n",
    "    test_df = test_df.loc[:, test_df.columns.get_level_values(\"Category\").isin(trendlines + [''])]\n",
    "\n",
    "    # Plot trendlines for each category\n",
    "    for category in trendlines:\n",
    "        # Extract population and values for the category\n",
    "        populations = test_df[\"Population\"].values\n",
    "        values = test_df[(amenity, category)].values.flatten()\n",
    "\n",
    "        # Perform linear regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(populations, values)\n",
    "\n",
    "        # Generate trendline\n",
    "        trendline = slope * populations + intercept\n",
    "        ax.plot(populations, trendline, color=colors[category], linestyle='--', label=f\"{category} Trendline\")\n",
    "        \n",
    "        # Print statistical results\n",
    "        print(f\"{amenity} - {category}:\")\n",
    "        print(f\"  Slope: {slope:.3f}\")\n",
    "        print(f\"  Intercept: {intercept:.3f}\")\n",
    "        print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "        print(f\"  P-value: {p_value:.3e}\")\n",
    "        print(f\"  Std Err: {std_err:.3f}\")\n",
    "    \n",
    "    # Customize the subplot\n",
    "    ax.set_title(f\"{amenity.capitalize()}\")\n",
    "    ax.set_xlabel(\"Population (Thousands)\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Count\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_distance = {\n",
    "    \"newyork\": 2790,\n",
    "    \"losangeles\": 0,\n",
    "    \"chicago\": 2015,\n",
    "    \"houston\": 1540,\n",
    "    \"phoenix\": 370,\n",
    "    \"philadelphia\": 2720,\n",
    "    \"sanantonio\": 1350,\n",
    "    \"sandiego\": 120,\n",
    "    \"dallas\": 1435,\n",
    "    \"jacksonville\": 2410,\n",
    "    \"austin\": 1375,\n",
    "    \"fortworth\": 1410,\n",
    "    \"sanjose\": 340,\n",
    "    \"columbus\": 2240,\n",
    "    \"charlotte\": 2430,\n",
    "    \"indianapolis\": 2060,\n",
    "    \"sanfrancisco\": 380,\n",
    "    \"seattle\": 960,\n",
    "    \"denver\": 1020,\n",
    "    \"oklahomacity\": 1320,\n",
    "    \"nashville\": 2100,\n",
    "    \"washington\": 2680,\n",
    "    \"elpaso\": 800,\n",
    "    \"lasvegas\": 270,\n",
    "    \"boston\": 2990,\n",
    "    \"detroit\": 2280,\n",
    "    \"portland\": 970,\n",
    "    \"louisville\": 2120,\n",
    "    \"memphis\": 1815,\n",
    "    \"baltimore\": 2690,\n",
    "    \"milwaukee\": 2040,\n",
    "    \"albuquerque\": 790,\n",
    "    \"tucson\": 485,\n",
    "    \"fresno\": 230,\n",
    "    \"sacramento\": 380,\n",
    "    \"mesa\": 370,\n",
    "    \"atlanta\": 2180,\n",
    "    \"kansascity\": 1600,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add founding date as a new column\n",
    "df_pivot[\"Distance\"] = df_pivot.index.get_level_values(\"City\").map(cities_to_distance)\n",
    "\n",
    "# Sort by the founding date\n",
    "df_sorted = df_pivot.reset_index().sort_values(by=\"Distance\").set_index([\"City\"])\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Initialize subplots for the three amenities\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# List of amenities (metrics)\n",
    "amenities = [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]\n",
    "\n",
    "# Colors for each category\n",
    "colors = {\"CCU\": \"blue\", \"CCD\": \"green\", \"LN\": \"orange\", \"LP\": \"purple\"}\n",
    "\n",
    "# Iterate over each amenity to create a subplot\n",
    "for i, amenity in enumerate(amenities):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    # Filter data for the current amenity\n",
    "    test_df = df_sorted.loc[:, df_sorted.columns.get_level_values('Metric').isin([amenity, 'Distance'])]\n",
    "    test_df = test_df.loc[:, test_df.columns.get_level_values(\"Category\").isin(trendlines + [''])]\n",
    "    \n",
    "    # Plot trendlines for each category\n",
    "    for category in trendlines:\n",
    "        # Extract founding dates (Distance) and values for the category\n",
    "        founding_dates = test_df[\"Distance\"].values\n",
    "        values = test_df[(amenity, category)].values.flatten()\n",
    "\n",
    "        # Perform linear regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(founding_dates, values)\n",
    "        \n",
    "        # Generate trendline\n",
    "        trendline = slope * founding_dates + intercept\n",
    "        ax.plot(founding_dates, trendline, color=colors[category], linestyle='--', label=f\"{category} Trendline\")\n",
    "        \n",
    "        # Display statistical information in console\n",
    "        print(f\"{amenity} - {category}:\")\n",
    "        print(f\"  Slope: {slope:.3f}\")\n",
    "        print(f\"  Intercept: {intercept:.3f}\")\n",
    "        print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "        print(f\"  P-value: {p_value:.3e}\")\n",
    "        print(f\"  Std Err: {std_err:.3f}\")\n",
    "    \n",
    "    # Customize the subplot\n",
    "    ax.set_title(f\"{amenity.capitalize()}\")\n",
    "    ax.set_xlabel(\"Distance (Miles)\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Count\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Census table B19083\n",
    "cities_to_gini = {\n",
    "    \"newyork\": 0.515,\n",
    "    \"losangeles\": 0.4879,\n",
    "    \"chicago\": 0.4798,\n",
    "    \"houston\": 0.4809,\n",
    "    \"phoenix\": 0.4564,\n",
    "    \"philadelphia\": 0.4824,\n",
    "    \"sanantonio\": 0.4591,\n",
    "    \"sandiego\": 0.4587,\n",
    "    \"dallas\": 0.4664,\n",
    "    \"jacksonville\": 0.4746,\n",
    "    \"austin\": 0.4734,\n",
    "    \"fortworth\": 0.4664,\n",
    "    \"sanjose\": 0.4811,\n",
    "    \"columbus\": 0.4641,\n",
    "    \"charlotte\": 0.4752,\n",
    "    \"indianapolis\": 0.4572,\n",
    "    \"sanfrancisco\": 0.4985,\n",
    "    \"seattle\": 0.4688,\n",
    "    \"denver\": 0.4518,\n",
    "    \"oklahomacity\": 0.4734,\n",
    "    \"nashville\": 0.4624,\n",
    "    \"washington\": 0.4472,\n",
    "    \"elpaso\": 0.4652,\n",
    "    \"lasvegas\": 0.4657,\n",
    "    \"boston\": 0.4836,\n",
    "    \"detroit\": 0.4738,\n",
    "    \"portland\": 0.4490,\n",
    "    \"louisville\": 0.4651,\n",
    "    \"memphis\": 0.4756,\n",
    "    \"baltimore\": 0.4623,\n",
    "    \"milwaukee\": 0.4771,\n",
    "    \"albuquerque\": 0.46,\n",
    "    \"tucson\": 0.4667,\n",
    "    \"fresno\": 0.4695,\n",
    "    \"sacramento\": 0.4510,\n",
    "    \"mesa\": 0.4564,\n",
    "    \"atlanta\": 0.4679,\n",
    "    \"kansascity\": 0.4507,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add founding date as a new column\n",
    "df_pivot[\"Gini\"] = df_pivot.index.get_level_values(\"City\").map(cities_to_gini)\n",
    "\n",
    "# Sort by the founding date\n",
    "df_sorted = df_pivot.reset_index().sort_values(by=\"Gini\").set_index([\"City\"])\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "\n",
    "# Initialize subplots for the three amenities\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "# List of amenities (metrics)\n",
    "amenities = [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]\n",
    "\n",
    "# Colors for each category\n",
    "colors = {\"CCU\": \"blue\", \"CCD\": \"green\", \"LN\": \"orange\", \"LP\": \"purple\"}\n",
    "\n",
    "# Iterate over each amenity to create a subplot\n",
    "for i, amenity in enumerate(amenities):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    # Filter data for the current amenity\n",
    "    test_df = df_sorted.loc[:, df_sorted.columns.get_level_values('Metric').isin([amenity, 'Gini'])]\n",
    "    test_df = test_df.loc[:, test_df.columns.get_level_values(\"Category\").isin(trendlines + [''])]\n",
    "    \n",
    "    # Plot trendlines for each category\n",
    "    for category in trendlines:\n",
    "        # Extract founding dates (Distance) and values for the category\n",
    "        founding_dates = test_df[\"Gini\"].values\n",
    "        values = test_df[(amenity, category)].values.flatten()\n",
    "\n",
    "        # Perform linear regression\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(founding_dates, values)\n",
    "        \n",
    "        # Generate trendline\n",
    "        trendline = slope * founding_dates + intercept\n",
    "        ax.plot(founding_dates, trendline, color=colors[category], linestyle='--', label=f\"{category} Trendline\")\n",
    "        \n",
    "        # Display statistical information in console\n",
    "        print(f\"{amenity} - {category}:\")\n",
    "        print(f\"  Slope: {slope:.3f}\")\n",
    "        print(f\"  Intercept: {intercept:.3f}\")\n",
    "        print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "        print(f\"  P-value: {p_value:.3e}\")\n",
    "        print(f\"  Std Err: {std_err:.3f}\")\n",
    "    \n",
    "    # Customize the subplot\n",
    "    ax.set_title(f\"{amenity.capitalize()}\")\n",
    "    ax.set_xlabel(\"Gini Index\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"Count\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-OOD_env]",
   "language": "python",
   "name": "conda-env-.conda-OOD_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
