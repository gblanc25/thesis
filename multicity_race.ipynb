{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "import osmnx as ox\n",
    "from pyrosm import OSM\n",
    "from shapely.wkt import loads as wkt_loads\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROCESS FLOW\n",
    "\n",
    "1. Download census tract data from https://www2.census.gov/geo/tiger/TIGER2024/BG/; note id from https://www2.census.gov/geo/tiger/TIGER_RD18/STATE/, store in tl folder\n",
    "2. Download OSM data for a given state here https://download.geofabrik.de/north-america/us.html, save as osm/{state}-latest.osm.pbf\n",
    "3. Download ACS data from https://data.census.gov/table/ACSDT5Y2022.B02001; filter to \"every block group in {county of target city}; save as acs_{city} folder inside acs_income or acs_race, rename nothing inside\n",
    "4. Run the following code block to process data\n",
    "5. Add new city to list of cities in graph code, run graph code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_ids = {\n",
    "    \"newyork\": 36,\n",
    "    \"losangeles\": \"06\",\n",
    "    \"chicago\": 17,\n",
    "    \"houston\": 48,\n",
    "    \"phoenix\": \"04\",\n",
    "    \"philadelphia\": 42,\n",
    "    \"sanantonio\": 48,\n",
    "    \"sandiego\": \"06\",\n",
    "    \"dallas\": 48,\n",
    "    \"jacksonville\": 12,\n",
    "    \"austin\": 48,\n",
    "    \"fortworth\": 48, \n",
    "    \"sanjose\": \"06\",\n",
    "    \"columbus\": 39,\n",
    "    \"charlotte\": 37,\n",
    "    \"indianapolis\": 18,\n",
    "    \"sanfrancisco\": \"06\",\n",
    "    \"seattle\": 53,\n",
    "    \"denver\": \"08\",\n",
    "    \"oklahomacity\": 40,\n",
    "    \"nashville\": 47,\n",
    "    \"washington\": 11,\n",
    "    \"elpaso\": 48,\n",
    "    \"lasvegas\": 32,\n",
    "    \"boston\": 25,\n",
    "    \"detroit\": 26,\n",
    "    \"portland\": 41,\n",
    "    \"louisville\": 21,\n",
    "    \"memphis\": 47,\n",
    "    \"baltimore\": 24,\n",
    "    \"milwaukee\": 55,\n",
    "    \"albuquerque\": 35,\n",
    "    \"tucson\": \"04\",\n",
    "    \"fresno\": \"06\",\n",
    "    \"sacramento\": \"06\",\n",
    "    \"mesa\": \"04\", \n",
    "    \"atlanta\": 13,\n",
    "    \"kansascity\": 29\n",
    "}\n",
    "\n",
    "cities_to_states = {\n",
    "    \"newyork\": \"new-york\",\n",
    "    \"losangeles\": \"california\",\n",
    "    \"chicago\": \"illinois\",\n",
    "    \"houston\": \"texas\",\n",
    "    \"phoenix\": \"arizona\",\n",
    "    \"philadelphia\": \"pennsylvania\",\n",
    "    \"sanantonio\": \"texas\",\n",
    "    \"sandiego\": \"california\",\n",
    "    \"dallas\": \"texas\",\n",
    "    \"jacksonville\": \"florida\",\n",
    "    \"austin\": \"texas\",\n",
    "    \"fortworth\": \"texas\", \n",
    "    \"sanjose\": \"california\",\n",
    "    \"columbus\": \"ohio\",\n",
    "    \"charlotte\": \"north-carolina\",\n",
    "    \"indianapolis\": \"indiana\",\n",
    "    \"sanfrancisco\": \"california\",\n",
    "    \"seattle\": \"washington\",\n",
    "    \"denver\": \"colorado\",\n",
    "    \"oklahomacity\": \"oklahoma\",\n",
    "    \"nashville\": \"tennessee\",\n",
    "    \"washington\": \"district-of-columbia\",\n",
    "    \"elpaso\": \"texas\",\n",
    "    \"lasvegas\": \"nevada\",\n",
    "    \"boston\": \"massachusetts\",\n",
    "    \"detroit\": \"michigan\",\n",
    "    \"portland\": \"oregon\",\n",
    "    \"louisville\": \"kentucky\",\n",
    "    \"memphis\": \"tennessee\",\n",
    "    \"baltimore\": \"maryland\",\n",
    "    \"milwaukee\": \"wisconsin\",\n",
    "    \"albuquerque\": \"new-mexico\",\n",
    "    \"tucson\": \"arizona\",\n",
    "    \"fresno\": \"california\",\n",
    "    \"sacramento\": \"california\",\n",
    "    \"mesa\": \"arizona\", \n",
    "    \"atlanta\": \"georgia\",\n",
    "    \"kansascity\": \"missouri\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_geojson(city, formal_city):\n",
    "    \n",
    "    # Access locally stored OSM files\n",
    "    city_fp = f\"osm/{cities_to_states[city]}-latest.osm.pbf\"\n",
    "\n",
    "    osm = OSM(city_fp)\n",
    "\n",
    "    city_boundary = ox.geocode_to_gdf(f\"{formal_city}, USA\")\n",
    "    print(\"Boundary defined\")\n",
    "\n",
    "    city_geom = city_boundary.geometry.iloc[0]\n",
    "\n",
    "    tags = {\n",
    "        \"amenity\": [\"library\", \"fire_station\", \"bank\", \"place_of_worship\", \"pharmacy\", \"social_facility\", \"police\", \"community_centre\"],\n",
    "        \"leisure\": [\"park\"],\n",
    "        \"building\": [\"school\", \"hospital\", \"residential\", \"house\", \"apartments\"],\n",
    "        \"landuse\": [\"residential\"],\n",
    "        \"shop\": [\"supermarket\"],\n",
    "        \"highway\": [\"bus_stop\"]\n",
    "    }\n",
    "\n",
    "    gdf = osm.get_data_by_custom_criteria(custom_filter=tags, filter_type=\"keep\")\n",
    "\n",
    "    # Filter data to keep only those within the city boundary\n",
    "    if gdf is not None and not gdf.empty:\n",
    "        try:\n",
    "            gdf = gdf[gdf.geometry.is_valid]\n",
    "            gdf = gdf[gdf.intersects(city_geom)]\n",
    "            gdf = gdf.drop(columns=[\"id\", \"timestamp\"], errors=\"ignore\")\n",
    "            print(\"Data extracted successfully\")\n",
    "        except:\n",
    "            invalid = gdf.loc[~gdf.geometry.is_valid]\n",
    "            print(invalid)\n",
    "\n",
    "        # Save to a GeoJSON file\n",
    "        output_fp = f\"buildings/{city}_buildings.geojson\"\n",
    "        gdf.to_file(output_fp, driver=\"GeoJSON\")\n",
    "        print(f\"GeoJSON saved at {output_fp}\")\n",
    "    else:\n",
    "        print(\"No data extracted for the specified tags.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# GENERALIZABLE FUNCTION\n",
    "\n",
    "def process_data(city, id):\n",
    "\n",
    "    census_tracts = gpd.read_file(f'tl/tl_2024_{id}_bg/tl_2024_{id}_bg.shp')\n",
    "    \n",
    "    # Load the OSM data (make sure it includes location data as points or polygons)\n",
    "    osm_data = gpd.read_file(f'buildings/{city}_buildings.geojson')\n",
    "        \n",
    "\n",
    "    # Reproject if needed (make sure both are in the same CRS)\n",
    "    osm_data = osm_data.to_crs(census_tracts.crs)\n",
    "    \n",
    "    # Perform spatial join\n",
    "    osm_with_geoid = gpd.sjoin(osm_data, census_tracts[['GEOID', 'geometry']], how='left')\n",
    "    \n",
    "    # Load the ACS data\n",
    "    acs_data = pd.read_csv(f\"acs/acs_{city}/ACSDT5Y2022.B19013-Data.csv\")\n",
    "    \n",
    "    acs_data = acs_data.rename(columns={\"B02001_002E\": \"White\"})\n",
    "    acs_data = acs_data.rename(columns={\"B02001_001E\": \"Total\"})\n",
    "    acs_data = acs_data.drop(0)\n",
    "    acs_data[\"WhitePerc\"] = acs_data[\"White\"].astype(int) / acs_data[\"Total\"].astype(int)\n",
    "    acs_data = acs_data[['GEO_ID', 'WhitePerc']]\n",
    "    acs_data[\"GEO_ID\"] = acs_data[\"GEO_ID\"].apply(lambda x: x[9:])\n",
    "    acs_data[\"WhitePerc\"] = acs_data[\"WhitePerc\"].apply(lambda x: float('nan') if x == \"-\" else x)\n",
    "    \n",
    "    acs_race = acs_data.astype(str)\n",
    "    acs_race = acs_race.dropna(subset=[\"WhitePerc\"])\n",
    "    \n",
    "    # Now merge the data\n",
    "    combined_with_race = osm_with_geoid.merge(acs_race, left_on=\"GEOID\", right_on=\"GEO_ID\", how=\"left\")\n",
    "\n",
    "    combined_with_race = combined_with_race.dropna(subset=[\"WhitePerc\"])\n",
    "        \n",
    "    return combined_with_race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cities = cities_to_ids.keys()\n",
    "\n",
    "formal_cities = [\"New York, New York\", \"Los Angeles, California\", \"Chicago, Illinois\", \"Houston, Texas\", \"Phoenix, Arizona\", \"Philadelphia, Pennsylvania\", \"San Antonio, Texas\", \"San Diego, California\",\n",
    "                 \"Dallas, Texas\", \"Jacksonville, Florida\", \"Austin, Texas\", \"Fort Worth, Texas\", \"San Jose, California\", \"Columbus, Ohio\", \"Charlotte, North Carolina\", \"Indianapolis, Indiana\", \"San Francisco, California\",\n",
    "                 \"Seattle, Washington\", \"Denver, Colorado\", \"Oklahoma City, Oklahoma\", \"Nashville, Tennessee\", \"Washington, District of Columbia\", \"El Paso, Texas\", \"Las Vegas, Nevada\", \"Boston, Massachusetts\",\n",
    "                 \"Detroit, Michigan\", \"Portland, Oregon\", \"Louisville, Kentucky\", \"Memphis, Tennessee\", \"Baltimore, Maryland\", \"Milwaukee, Wisconsin\", \"Albuquerque, New Mexico\", \"Tucson, Arizona\", \"Fresno, California\",\n",
    "                 \"Sacramento, California\", \"Mesa, Arizona\", \"Atlanta, Georgia\", \"Kansas City, Missouri\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "\n",
    "for city, formal_city in zip(cities_to_ids.keys(), formal_cities):\n",
    "    print(\"starting\", city, formal_city)\n",
    "    if not os.path.exists(f\"buildings/{city}_buildings.geojson\"):\n",
    "        convert_to_geojson(city, formal_city)\n",
    "    dataframes[city] = process_data(city, cities_to_ids[city])\n",
    "    print(city, \"complete! length\", len(dataframes[city]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This num_divisions parameter is very important; it impacts the behavior of all future graphs. If you want graphs to appear as tertiles\n",
    "(as in the lower correlative analysis), it should be 3; to produce proper graphs for distributions, it should be 5. This can be customized\n",
    "to work with any reasonable number.\n",
    "'''\n",
    "\n",
    "def category_normalized_counts_by_race(categories, city_names, num_divisions=3):\n",
    "    all_results = {}\n",
    "\n",
    "    for city_name in city_names:\n",
    "        \n",
    "        print(city_name)\n",
    "        \n",
    "        df = dataframes[city_name]\n",
    "        \n",
    "        df.drop(['race_range'], axis=1, errors='ignore', inplace=True)\n",
    "        \n",
    "        df[\"WhitePerc\"] = pd.to_numeric(df[\"WhitePerc\"], errors=\"coerce\")\n",
    "        \n",
    "        # Drop rows where WhitePerc is NaN after conversion\n",
    "        df = df.dropna(subset=[\"WhitePerc\"])\n",
    "\n",
    "        # Step 1: Calculate race tertiles based on unique block groups\n",
    "        unique_bg = df.drop_duplicates(subset=[\"GEO_ID\"])[[\"GEO_ID\", \"WhitePerc\"]]\n",
    "    \n",
    "        unique_bg[\"race_range\"], race_bins = pd.qcut(\n",
    "            unique_bg[\"WhitePerc\"], \n",
    "            q=num_divisions, \n",
    "            retbins=True, \n",
    "            labels=[f\"Q{i+1}\" for i in range(num_divisions)]\n",
    "        )\n",
    "        \n",
    "        # Step 2: Count the number of block groups in each race range\n",
    "        block_group_counts = unique_bg.groupby(\"race_range\", observed=False).size().reset_index(name=\"bg_count\") \n",
    "\n",
    "        # Step 3: Merge race range back to the main DataFrame\n",
    "        df = pd.merge(df, unique_bg[[\"GEO_ID\", \"race_range\"]], on=\"GEO_ID\", how=\"left\")\n",
    "        city_results = {}\n",
    "        for category, items in categories.items():\n",
    "            category_results = {}\n",
    "            for item in items:\n",
    "                try:\n",
    "                    filtered_data = df[df[category] == item]\n",
    "\n",
    "                    counts_by_race = filtered_data.groupby(\"race_range\", observed=False).size().reset_index(name=\"count\")\n",
    "\n",
    "                    counts_with_bg = pd.merge(counts_by_race, block_group_counts, on=\"race_range\", how=\"left\")\n",
    "\n",
    "                    # Calculate the normalized count (service count per block group) across cities\n",
    "                    counts_with_bg[\"normalized_count\"] = counts_with_bg[\"count\"] / counts_with_bg[\"bg_count\"]\n",
    "\n",
    "                    # Store the results for each item in the current category\n",
    "                    category_results[item] = counts_with_bg[[\"race_range\", \"normalized_count\"]]\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            city_results[category] = category_results\n",
    "        \n",
    "        all_results[city_name] = city_results\n",
    "        dataframes[city_name] = df\n",
    "    \n",
    "    return all_results, race_bins\n",
    "\n",
    "# Take means of all trend lines to aggregate\n",
    "def aggregate_city_trendlines(results, categories):\n",
    "    aggregated_results = {}\n",
    "\n",
    "    for category, items in categories.items():\n",
    "        aggregated_results[category] = {}\n",
    "        \n",
    "        for item in items:\n",
    "            all_city_data = []\n",
    "            \n",
    "            for city_name, city_data in results.items():\n",
    "                try:\n",
    "                    all_city_data.append(city_data[category][item])\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            combined_df = pd.concat(all_city_data)\n",
    "            \n",
    "            # Compute the average normalized count per race tertile\n",
    "            aggregated_df = combined_df.groupby(\"race_range\", observed=False)[\"normalized_count\"].mean().reset_index()\n",
    "            \n",
    "            aggregated_results[category][item] = aggregated_df\n",
    "\n",
    "    return aggregated_results\n",
    "\n",
    "categories = {\n",
    "    \"amenity\": [\"library\", \"fire_station\", \"bank\", \"place_of_worship\", \"pharmacy\", \"social_facility\", \"police\"],\n",
    "    \"leisure\": [\"park\"],\n",
    "    \"building\": [\"school\", \"hospital\"],\n",
    "    \"shop\": [\"supermarket\"],\n",
    "    \"highway\": [\"bus_stop\"]\n",
    "}\n",
    "\n",
    "city_names = list(cities_to_ids.keys())\n",
    "\n",
    "results, race_bins = category_normalized_counts_by_race(categories, city_names)\n",
    "\n",
    "aggregated_results = aggregate_city_trendlines(results, categories)\n",
    "\n",
    "total_items = sum(len(items) for items in categories.values())\n",
    "cols = 3  # Number of columns in the grid\n",
    "rows = math.ceil(total_items / cols)  # Calculate the number of rows required\n",
    "\n",
    "fig, axs = plt.subplots(rows, cols, figsize=(15, rows * 5))\n",
    "\n",
    "plot_index = 0\n",
    "\n",
    "# Loop through each category and item\n",
    "for category, items in categories.items():\n",
    "    for item in items:\n",
    "        # Determine the current row and column based on plot_index\n",
    "        row = plot_index // cols\n",
    "        col = plot_index % cols\n",
    "        ax = axs[row, col] if rows > 1 else axs[col]  \n",
    "\n",
    "        # Plot aggregated data (single trendline per category)\n",
    "        data = aggregated_results[category][item]\n",
    "        ax.plot(data[\"race_range\"], data[\"normalized_count\"], marker='o', linestyle='-', label=f\"{item} Trendline\", color=\"black\")\n",
    "\n",
    "        ax.set_title(f\"{category.capitalize()}: {item.capitalize()}\")\n",
    "        ax.set_xlabel(\"Percent White Tertile\")\n",
    "        ax.set_ylabel(\"Average Amenity Count : Block Group Ratio\")\n",
    "        ax.legend()\n",
    "        \n",
    "        plot_index += 1\n",
    "\n",
    "# Hide any unused subplots if the grid has extra cells\n",
    "for j in range(plot_index, rows * cols):\n",
    "    fig.delaxes(axs[j // cols, j % cols])\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to fit the main title\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = {\n",
    "    \"quality-of-life\": {\n",
    "        \"amenity\": [\"library\", \"place_of_worship\"],\n",
    "        \"building\": [],\n",
    "        \"leisure\": [\"park\"],\n",
    "        \"shop\": [\"supermarket\"],\n",
    "        \"highway\": []\n",
    "    },\n",
    "    \"economic-mobility\": {\n",
    "        \"amenity\": [\"social_facility\", \"bank\"],\n",
    "        \"building\": [\"school\"],\n",
    "        \"leisure\": [],\n",
    "        \"shop\": [],\n",
    "        \"highway\": [\"bus_stop\"]\n",
    "    },\n",
    "    \"health-and-safety\": {\n",
    "        \"amenity\": [\"fire_station\", \"police\", \"pharmacy\"],\n",
    "        \"building\": [\"hospital\"],\n",
    "        \"leisure\": [],\n",
    "        \"shop\": [],\n",
    "        \"highway\": []\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def calculate_min_distances_block_group(df, city_name, categories, block_group_column=\"GEO_ID\"):\n",
    "    projected_crs = \"EPSG:3857\"\n",
    "    results = {}\n",
    "\n",
    "    if df[\"geometry\"].dtype == \"object\":\n",
    "        df[\"geometry\"] = df[\"geometry\"].apply(\n",
    "            lambda geom: wkt_loads(geom) if isinstance(geom, str) else geom\n",
    "        )\n",
    "\n",
    "    df = gpd.GeoDataFrame(df, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "    # Reproject to the projected CRS for distance calculations\n",
    "    df = df.to_crs(projected_crs)\n",
    "    \n",
    "    city_id = cities_to_ids[city_name]\n",
    "    census_tracts = gpd.read_file(f'tl/tl_2024_{city_id}_bg/tl_2024_{city_id}_bg.shp')\n",
    "    census_tracts = census_tracts.to_crs(epsg = \"4269\")\n",
    "    census_tracts = census_tracts[[\"GEOID\", \"geometry\"]]\n",
    "\n",
    "    block_group_areas = census_tracts.dissolve(by=\"GEOID\")\n",
    "\n",
    "    # Calculate area for each dissolved region\n",
    "    \n",
    "    block_group_areas = block_group_areas['geometry'].area.reset_index()\n",
    "\n",
    "    block_group_areas.rename(columns={0: \"block_group_area\"}, inplace=True)\n",
    "    \n",
    "    block_group_areas.rename(columns={\"GEOID\": \"GEO_ID\"}, inplace=True)\n",
    "    \n",
    "    max_val = block_group_areas[\"block_group_area\"].max()\n",
    "    block_group_areas[\"norm_factor\"] = 1 - (np.log1p(block_group_areas[\"block_group_area\"]) / np.log1p(max_val))\n",
    "    \n",
    "    for category, items in categories.items():\n",
    "        category_results = {}\n",
    "        for item in items:\n",
    "            # Filter amenities by category and item\n",
    "            amenities = df[df[category] == item]\n",
    "            amenities = gpd.GeoDataFrame(amenities, geometry=\"geometry\").to_crs(projected_crs)\n",
    "            amenities[\"geometry\"] = amenities[\"geometry\"].apply(\n",
    "                lambda geom: geom.centroid if geom.geom_type == \"Polygon\" else geom\n",
    "            )\n",
    "\n",
    "            residential_buildings = df[\n",
    "                (df[\"building\"].isin([\"residential\", \"house\", \"apartments\"])) |  \n",
    "                (df[\"landuse\"] == \"residential\")  \n",
    "            ].copy()\n",
    "\n",
    "            # Randomly sample one residence per block group\n",
    "            sampled_residences = residential_buildings.groupby(block_group_column).sample(n=1, random_state=42)\n",
    "\n",
    "            # Compute minimum distance from sampled residences to each amenity\n",
    "            sampled_residences[f\"min_dist_to_{item}\"] = sampled_residences[\"geometry\"].apply(\n",
    "                lambda geom: amenities.distance(geom).min() if not amenities.empty else np.nan\n",
    "            )\n",
    "\n",
    "            # Merge with block group area data\n",
    "            sampled_residences = sampled_residences.merge(block_group_areas, on=block_group_column, how=\"left\")\n",
    "\n",
    "            # Normalize distances by block group size\n",
    "            sampled_residences[f\"normalized_dist_to_{item}\"] = (\n",
    "                sampled_residences[f\"min_dist_to_{item}\"] / np.sqrt(sampled_residences[\"block_group_area\"])\n",
    "            )\n",
    "            \n",
    "            scaling_factor = sampled_residences[f\"min_dist_to_{item}\"].mean() / sampled_residences[f\"normalized_dist_to_{item}\"].mean()\n",
    "            sampled_residences[f\"normalized_dist_to_{item}\"] *= scaling_factor\n",
    "\n",
    "            # Calculate average normalized distance per block group\n",
    "            block_group_avg = sampled_residences.groupby(block_group_column, observed=False).agg(\n",
    "                avg_dist=(\"normalized_dist_to_\" + item, \"mean\"),\n",
    "                race_range=(\"race_range\", \"first\")\n",
    "            ).reset_index()\n",
    "\n",
    "            # Aggregate by race range (ignoring NaN values)\n",
    "            race_range_avg = block_group_avg.groupby(\"race_range\", observed=False).agg(\n",
    "                avg_dist=(\"avg_dist\", \"mean\")\n",
    "            ).reset_index()\n",
    "\n",
    "            category_results[item] = race_range_avg\n",
    "\n",
    "        results[category] = category_results\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def aggregate_distance_trendlines(results_by_city, categories):\n",
    "    aggregated_results = {}\n",
    "\n",
    "    for category, items in categories.items():\n",
    "        aggregated_results[category] = {}\n",
    "\n",
    "        for item in items:\n",
    "            all_city_data = []\n",
    "\n",
    "            for city_name, city_results in results_by_city.items():\n",
    "                try:\n",
    "                    city_data = city_results[category][item]\n",
    "                    city_data = city_data.replace([np.inf, -np.inf], np.nan)  \n",
    "                    all_city_data.append(city_data)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            if all_city_data:\n",
    "                combined_df = pd.concat(all_city_data, ignore_index=True)\n",
    "\n",
    "                # Compute the average distance per race tertile (excluding NaNs)\n",
    "                aggregated_df = combined_df.groupby(\"race_range\", observed=False)[\"avg_dist\"].mean().reset_index()\n",
    "\n",
    "                aggregated_results[category][item] = aggregated_df\n",
    "\n",
    "    return aggregated_results\n",
    "\n",
    "def bootstrap_aggregate_distance_trendlines(results_by_city, categories, n_iterations=500, sample_size=None):\n",
    "    aggregated_results = {}\n",
    "    all_city_names = list(results_by_city.keys())\n",
    "    if sample_size is None:\n",
    "        sample_size = len(all_city_names)\n",
    "        \n",
    "    for category, items in categories.items():\n",
    "        aggregated_results[category] = {}\n",
    "        \n",
    "        for item in items:\n",
    "            bootstrap_results = []\n",
    "            \n",
    "            for i in range(n_iterations):\n",
    "                sampled_cities = np.random.choice(all_city_names, size=sample_size, replace=True)\n",
    "                all_city_data = []\n",
    "                \n",
    "                for city in sampled_cities:\n",
    "                    city_results = results_by_city.get(city, None)\n",
    "                    if city_results is None:\n",
    "                        continue\n",
    "                    try:\n",
    "                        city_data = city_results[category][item]\n",
    "                        city_data = city_data.replace([np.inf, -np.inf], np.nan)\n",
    "                        all_city_data.append(city_data)\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "                \n",
    "                if all_city_data:\n",
    "                    combined_df = pd.concat(all_city_data, ignore_index=True)\n",
    "                    group_means = combined_df.groupby(\"race_range\", observed=False)[\"avg_dist\"].mean().reset_index()\n",
    "                    bootstrap_results.append(group_means)\n",
    "            \n",
    "            all_race_ranges = sorted(set().union(*(set(df['race_range']) for df in bootstrap_results)))\n",
    "            summary_data = []\n",
    "            \n",
    "            for race_range in all_race_ranges:\n",
    "                values = []\n",
    "                for df in bootstrap_results:\n",
    "                    val = df.loc[df[\"race_range\"] == race_range, \"avg_dist\"]\n",
    "                    if not val.empty:\n",
    "                        values.append(val.iloc[0])\n",
    "                if values:\n",
    "                    mean_val = np.mean(values)\n",
    "                    lower = np.percentile(values, 2.5)\n",
    "                    upper = np.percentile(values, 97.5)\n",
    "                    summary_data.append({\n",
    "                        \"race_range\": race_range,\n",
    "                        \"avg_dist\": mean_val,\n",
    "                        \"ci_lower\": lower,\n",
    "                        \"ci_upper\": upper\n",
    "                    })\n",
    "            \n",
    "            summary_df = pd.DataFrame(summary_data)\n",
    "            aggregated_results[category][item] = summary_df\n",
    "            \n",
    "    return aggregated_results\n",
    "\n",
    "def prepare_distance_data(results_dict, categories):\n",
    "    results = {}\n",
    "    for category, items in categories.items():\n",
    "        category_results = {}\n",
    "        for item in items:\n",
    "            if item in results_dict[category]:\n",
    "                category_results[item] = results_dict[category][item]\n",
    "        results[category] = category_results\n",
    "    return results\n",
    "\n",
    "def plot_aggregated_distance_graphs(aggregated_results, categories):\n",
    "    total_items = sum(len(items) for items in categories.values())\n",
    "    cols = 3\n",
    "    rows = math.ceil(total_items / cols)\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, rows * 5))\n",
    "    plot_index = 0\n",
    "    \n",
    "    rows_cols_by_amenity = {\n",
    "        \"pharmacy\": (0, 2, \"Health & Safety\"),\n",
    "        \"police\": (1, 2, \"Health & Safety\"),\n",
    "        \"hospital\": (2, 2, \"Health & Safety\"),\n",
    "        \"fire_station\": (3, 2, \"Health & Safety\"),\n",
    "        \"school\": (0, 1, \"Economic Mobility\"),\n",
    "        \"bank\": (1, 1, \"Economic Mobility\"),\n",
    "        \"social_facility\": (2, 1, \"Economic Mobility\"),\n",
    "        \"bus_stop\": (3, 1, \"Economic Mobility\"),\n",
    "        \"park\": (0, 0, \"Quality of Life\"),\n",
    "        \"supermarket\": (1, 0, \"Quality of Life\"),\n",
    "        \"place_of_worship\": (2, 0, \"Quality of Life\"),\n",
    "        \"library\": (3, 0, \"Quality of Life\")\n",
    "    }\n",
    "\n",
    "    for category, items in categories.items():\n",
    "        for item in items:\n",
    "            row, col, cat = rows_cols_by_amenity.get(item, (0,0))\n",
    "            ax = axs[row, col] if rows > 1 else axs[col]\n",
    "            \n",
    "            if item == \"social_facility\":\n",
    "                name = \"Social Facility\"\n",
    "            elif item == \"place_of_worship\":\n",
    "                name = \"Place of Worship\"\n",
    "            elif item == \"fire_station\":\n",
    "                name = \"Fire Station\"\n",
    "            elif item == \"bus_stop\":\n",
    "                name = \"Bus Stop\"\n",
    "            else:\n",
    "                name = item.capitalize()\n",
    "\n",
    "            data = aggregated_results.get(category, {}).get(item, None)\n",
    "            if data is not None and not data.empty:\n",
    "                \n",
    "                if col == 0:\n",
    "                    color = \"green\"\n",
    "                elif col == 1:\n",
    "                    color = \"brown\"\n",
    "                else:\n",
    "                    color = \"blue\"\n",
    "                \n",
    "                x = data[\"race_range\"]\n",
    "                y = data[\"avg_dist\"].values\n",
    "                \n",
    "                ax.plot(x, y, marker='o', linestyle='-', color=color, label=f\"{item} Trendline\")\n",
    "                \n",
    "                ax.fill_between(\n",
    "                    x, \n",
    "                    data[\"ci_lower\"], \n",
    "                    data[\"ci_upper\"], \n",
    "                    color=color, \n",
    "                    alpha=0.05  \n",
    "                )\n",
    "                \n",
    "                x_numeric = np.arange(len(data))\n",
    "                slope, intercept, r_value, p_value, std_err = linregress(x_numeric, y)\n",
    "                print(f\"ðŸ“Š {category.capitalize()} - {item}:\")\n",
    "                print(f\"  Slope: {slope:.3f}\")\n",
    "                print(f\"  Intercept: {intercept:.3f}\")\n",
    "                print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "                print(f\"  P-value: {p_value:.3e}\")\n",
    "                print(f\"  Std Err: {std_err:.3f}\\n\")\n",
    "            \n",
    "            ax.set_title(f\"{cat}: {name}\")\n",
    "            ax.set_xlabel(\"Percent White\")\n",
    "            ax.set_ylabel(\"Average Adjusted Distance (Meters)\")\n",
    "            plot_index += 1\n",
    "\n",
    "    # Remove any empty subplots\n",
    "    for j in range(plot_index, rows * cols):\n",
    "        fig.delaxes(axs[j // cols, j % cols])\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# List of city dataframes\n",
    "city_names = list(cities_to_ids.keys())\n",
    "\n",
    "with open(\"results_by_city_5.pkl\", \"rb\") as f:\n",
    "    results_by_city = pickle.load(f)\n",
    "\n",
    "# Aggregate data for trendlines (ignoring missing amenities)\n",
    "with open(\"aggregated_results_race_5.pkl\", \"rb\") as f:\n",
    "    aggregated_results = pickle.load(f)\n",
    "\n",
    "plot_aggregated_distance_graphs(aggregated_results, categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classification_average_trendlines(aggregated_results, classifications):\n",
    "\n",
    "    classification_names = list(classifications.keys())\n",
    "    print(classification_names)\n",
    "    n_class = len(classification_names)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, n_class, figsize=(6 * n_class, 5), sharey=True)\n",
    "    if n_class == 1:\n",
    "        axs = [axs] \n",
    "    \n",
    "    flat_results = {}\n",
    "    for outer_key in aggregated_results:\n",
    "        for item, df in aggregated_results[outer_key].items():\n",
    "            flat_results[item] = df\n",
    "\n",
    "    for ax, cls in zip(axs, classification_names):\n",
    "        items = []\n",
    "        for group_type, item_list in classifications[cls].items():\n",
    "            items.extend(item_list)\n",
    "        \n",
    "        dfs = [flat_results[item] for item in items if item in flat_results]\n",
    "        \n",
    "        if not dfs:\n",
    "            ax.set_title(cls.replace(\"-\", \" \").capitalize() + \"\\n(No Data)\")\n",
    "            continue\n",
    "        \n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        summary_df = combined_df.groupby(\"race_range\", as_index=False)[[\"avg_dist\", \"ci_lower\", \"ci_upper\"]].mean()\n",
    "        summary_df = summary_df.sort_values(\"race_range\")\n",
    "        \n",
    "        if cls == \"quality-of-life\":\n",
    "            color = \"green\"\n",
    "            name = \"Quality of Life\"\n",
    "        elif cls == \"economic-mobility\":\n",
    "            color = \"brown\"\n",
    "            name = \"Economic Mobility\"\n",
    "        else:\n",
    "            color = \"blue\"\n",
    "            name = \"Health and Safety\"\n",
    "        \n",
    "        x = summary_df[\"race_range\"]\n",
    "        y = summary_df[\"avg_dist\"]\n",
    "        ax.plot(x, y, marker='o', linestyle='-', color=color, label=f\"{cls} trendline\")\n",
    "        \n",
    "        ax.set_title(name)\n",
    "        ax.set_xlabel(\"Percent White\")\n",
    "        ax.set_ylabel(\"Average Adjusted Distance (Meters)\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classification_average_trendlines(aggregated_results, classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_gini_index(results_by_city, categories, cities_to_gini):\n",
    "\n",
    "    aggregated_results = {}\n",
    "\n",
    "    for category, items in categories.items():\n",
    "        aggregated_results[category] = {}\n",
    "\n",
    "        for item in items:\n",
    "            city_data = []  \n",
    "\n",
    "            for city_name, city_results in results_by_city.items():\n",
    "                if city_name not in cities_to_gini:\n",
    "                    continue  \n",
    "\n",
    "                gini = cities_to_gini[city_name]  \n",
    "                \n",
    "                try:\n",
    "                    city_df = city_results[category][item]  \n",
    "                    city_df = city_df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "                    avg_distance = city_df[\"avg_dist\"].mean()  # Compute mean distance\n",
    "                    city_data.append({\"gini_index\": gini, \"avg_dist\": avg_distance})\n",
    "                except:\n",
    "                    continue  \n",
    "\n",
    "            if city_data:\n",
    "                df = pd.DataFrame(city_data)\n",
    "\n",
    "                lower_ci = np.percentile(df[\"avg_dist\"], 2.5) if len(df) > 1 else df[\"avg_dist\"].min()\n",
    "                upper_ci = np.percentile(df[\"avg_dist\"], 97.5) if len(df) > 1 else df[\"avg_dist\"].max()\n",
    "                \n",
    "                df[\"ci_lower\"] = lower_ci\n",
    "                df[\"ci_upper\"] = upper_ci\n",
    "\n",
    "                aggregated_results[category][item] = df\n",
    "\n",
    "    return aggregated_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gini_vs_distance(aggregated_results, categories):\n",
    "    total_items = sum(len(items) for items in categories.values())\n",
    "    cols = 3\n",
    "    rows = math.ceil(total_items / cols)\n",
    "\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=(15, rows * 5))\n",
    "    plot_index = 0\n",
    "    \n",
    "    rows_cols_by_amenity = {\n",
    "        \"pharmacy\": (0, 2, \"Health & Safety\"),\n",
    "        \"police\": (1, 2, \"Health & Safety\"),\n",
    "        \"hospital\": (2, 2, \"Health & Safety\"),\n",
    "        \"fire_station\": (3, 2, \"Health & Safety\"),\n",
    "        \"school\": (0, 1, \"Economic Mobility\"),\n",
    "        \"bank\": (1, 1, \"Economic Mobility\"),\n",
    "        \"social_facility\": (2, 1, \"Economic Mobility\"),\n",
    "        \"bus_stop\": (3, 1, \"Economic Mobility\"),\n",
    "        \"park\": (0, 0, \"Quality of Life\"),\n",
    "        \"supermarket\": (1, 0, \"Quality of Life\"),\n",
    "        \"place_of_worship\": (2, 0, \"Quality of Life\"),\n",
    "        \"library\": (3, 0, \"Quality of Life\")\n",
    "    }\n",
    "\n",
    "    for category, items in categories.items():\n",
    "        for item in items:\n",
    "            row, col, cat = rows_cols_by_amenity.get(item, (0, 0, \"Other\"))\n",
    "            ax = axs[row, col] if rows > 1 else axs[col]\n",
    "\n",
    "            # Get the aggregated data\n",
    "            data = aggregated_results.get(category, {}).get(item, None)\n",
    "            if data is not None and not data.empty:\n",
    "                # Sort data by Gini index\n",
    "                data = data.sort_values(\"gini_index\")\n",
    "\n",
    "                # Linear regression analysis\n",
    "                slope, intercept, r_value, p_value, std_err = linregress(data[\"gini_index\"], data[\"avg_dist\"])\n",
    "\n",
    "                # Generate regression line\n",
    "                x_vals = np.linspace(data[\"gini_index\"].min(), data[\"gini_index\"].max(), 100)\n",
    "                y_vals = slope * x_vals + intercept\n",
    "\n",
    "                # Assign color based on category\n",
    "                color = \"green\" if col == 0 else \"brown\" if col == 1 else \"blue\"\n",
    "\n",
    "                # Plot the regression line\n",
    "                ax.plot(x_vals, y_vals, linestyle=\"-\", color=color, label=f\"{item} Trendline\")\n",
    "\n",
    "                # Display regression statistics\n",
    "                ax.annotate(\n",
    "                    f\"Slope: {slope:.3f}\\nRÂ²: {r_value**2:.3f}\\nP-value: {p_value:.3e}\",\n",
    "                    xy=(0.05, 0.85), xycoords=\"axes fraction\", fontsize=10,\n",
    "                    bbox=dict(facecolor=\"white\", edgecolor=\"black\", boxstyle=\"round,pad=0.3\")\n",
    "                )\n",
    "\n",
    "                print(f\"ðŸ“Š {category.capitalize()} - {item}:\")\n",
    "                print(f\"  Slope: {slope:.3f}\")\n",
    "                print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "                print(f\"  P-value: {p_value:.3e}\")\n",
    "                print(f\"  Std Err: {std_err:.3f}\\n\")\n",
    "\n",
    "            ax.set_title(f\"{cat}: {item.capitalize()}\")\n",
    "            ax.set_xlabel(\"Gini Index\")\n",
    "            ax.set_ylabel(\"Average Adjusted Distance (Meters)\")\n",
    "            plot_index += 1\n",
    "\n",
    "    for j in range(plot_index, rows * cols):\n",
    "        fig.delaxes(axs[j // cols, j % cols])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results_gini = aggregate_by_gini_index(results_by_city, categories, cities_to_gini)\n",
    "plot_gini_vs_distance(aggregated_results_gini, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_category_by_gini(results_by_city, classifications, cities_to_gini):\n",
    "\n",
    "    aggregated_results = {}\n",
    "\n",
    "    for category, amenities in classifications.items():\n",
    "        category_data = []\n",
    "\n",
    "        for city_name, city_results in results_by_city.items():\n",
    "            if city_name not in cities_to_gini:\n",
    "                continue  \n",
    "\n",
    "            gini = cities_to_gini[city_name]  \n",
    "            city_avg_distances = []\n",
    "\n",
    "            for amenity_group, items in amenities.items():\n",
    "                for item in items:\n",
    "                    try:\n",
    "                        city_df = city_results[amenity_group][item]\n",
    "                        city_df = city_df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "                        avg_distance = city_df[\"avg_dist\"].mean()  \n",
    "                        city_avg_distances.append(avg_distance)\n",
    "                    except:\n",
    "                        continue  \n",
    "\n",
    "            if city_avg_distances:\n",
    "                category_avg_dist = np.mean(city_avg_distances)  \n",
    "                category_data.append({\"gini_index\": gini, \"avg_dist\": category_avg_dist})\n",
    "\n",
    "        if category_data:\n",
    "            df = pd.DataFrame(category_data)\n",
    "\n",
    "            lower_ci = np.percentile(df[\"avg_dist\"], 2.5) if len(df) > 1 else df[\"avg_dist\"].min()\n",
    "            upper_ci = np.percentile(df[\"avg_dist\"], 97.5) if len(df) > 1 else df[\"avg_dist\"].max()\n",
    "            \n",
    "            df[\"ci_lower\"] = lower_ci\n",
    "            df[\"ci_upper\"] = upper_ci\n",
    "\n",
    "            aggregated_results[category] = df\n",
    "\n",
    "    return aggregated_results\n",
    "\n",
    "aggregated_results = aggregate_category_by_gini(results_by_city, classifications, cities_to_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_category_trendlines(aggregated_results):\n",
    "\n",
    "    classification_names = list(aggregated_results.keys())\n",
    "    n_class = len(classification_names)\n",
    "\n",
    "    fig, axs = plt.subplots(1, n_class, figsize=(6 * n_class, 5), sharey=True)\n",
    "    if n_class == 1:\n",
    "        axs = [axs]  \n",
    "\n",
    "    for ax, cls in zip(axs, classification_names):\n",
    "        data = aggregated_results.get(cls, None)\n",
    "        if data is None or data.empty:\n",
    "            ax.set_title(cls.replace(\"-\", \" \").capitalize() + \"\\n(No Data)\")\n",
    "            continue\n",
    "\n",
    "        data = data.sort_values(\"gini_index\")\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(data[\"gini_index\"], data[\"avg_dist\"])\n",
    "\n",
    "        x_vals = np.linspace(data[\"gini_index\"].min(), data[\"gini_index\"].max(), 100)\n",
    "        y_vals = slope * x_vals + intercept\n",
    "\n",
    "        color_map = {\n",
    "            \"quality-of-life\": \"green\",\n",
    "            \"economic-mobility\": \"brown\",\n",
    "            \"health-safety\": \"blue\"\n",
    "        }\n",
    "        color = color_map.get(cls, \"blue\")\n",
    "\n",
    "        ax.plot(x_vals, y_vals, linestyle=\"-\", color=color, label=f\"{cls} Trendline\")\n",
    "\n",
    "        ax.annotate(\n",
    "            f\"Slope: {slope:.3f}\\nRÂ²: {r_value**2:.3f}\\nP-value: {p_value:.3e}\",\n",
    "            xy=(0.05, 0.85), xycoords=\"axes fraction\", fontsize=10,\n",
    "            bbox=dict(facecolor=\"white\", edgecolor=\"black\", boxstyle=\"round,pad=0.3\")\n",
    "        )\n",
    "\n",
    "        print(f\"ðŸ“Š {cls.capitalize()} Category:\")\n",
    "        print(f\"  Slope: {slope:.3f}\")\n",
    "        print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "        print(f\"  P-value: {p_value:.3e}\")\n",
    "        print(f\"  Std Err: {std_err:.3f}\\n\")\n",
    "\n",
    "        ax.set_title(cls.replace(\"-\", \" \").capitalize())\n",
    "        ax.set_xlabel(\"Gini Index\")\n",
    "        ax.set_ylabel(\"Average Adjusted Distance (Meters)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_category_trendlines(aggregated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Census table B19083\n",
    "cities_to_gini = {\n",
    "    \"newyork\": 0.5136,\n",
    "    \"losangeles\": 0.4928,\n",
    "    \"chicago\": 0.4817,\n",
    "    \"houston\": 0.4827,\n",
    "    \"phoenix\": 0.4534,\n",
    "    \"philadelphia\": 0.4828,\n",
    "    \"sanantonio\": 0.4600,\n",
    "    \"sandiego\": 0.4602,\n",
    "    \"dallas\": 0.4637,\n",
    "    \"jacksonville\": 0.4663,\n",
    "    \"austin\": 0.4563,\n",
    "    \"fortworth\": 0.4637,\n",
    "    \"sanjose\": 0.4743,\n",
    "    \"columbus\": 0.4583,\n",
    "    \"charlotte\": 0.4749,\n",
    "    \"indianapolis\": 0.4655,\n",
    "    \"sanfrancisco\": 0.4913,\n",
    "    \"seattle\": 0.4620,\n",
    "    \"denver\": 0.4482,\n",
    "    \"oklahomacity\": 0.4646,\n",
    "    \"nashville\": 0.4646,\n",
    "    \"washington\": 0.4472,\n",
    "    \"elpaso\": 0.4663,\n",
    "    \"lasvegas\": 0.4706,\n",
    "    \"boston\": 0.4847,\n",
    "    \"detroit\": 0.4726,\n",
    "    \"portland\": 0.4491,\n",
    "    \"louisville\": 0.4624,\n",
    "    \"memphis\": 0.4897,\n",
    "    \"baltimore\": 0.4603,\n",
    "    \"milwaukee\": 0.4710,\n",
    "    \"albuquerque\": 0.4668,\n",
    "    \"tucson\": 0.4679,\n",
    "    \"fresno\": 0.4730,\n",
    "    \"sacramento\": 0.4556,\n",
    "    \"mesa\": 0.4564,\n",
    "    \"atlanta\": 0.4676,\n",
    "    \"kansascity\": 0.4518,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_names = cities_to_ids.keys()\n",
    "\n",
    "class_counts = {}\n",
    "\n",
    "for city in city_names:\n",
    "    class_counts[city] = {\n",
    "        \"CCU\": {\n",
    "            \"quality-of-life\": 0,\n",
    "            \"economic-mobility\": 0,\n",
    "            \"health-and-safety\": 0,\n",
    "        },\n",
    "        \"CCD\": {\n",
    "            \"quality-of-life\": 0,\n",
    "            \"economic-mobility\": 0,\n",
    "            \"health-and-safety\": 0,\n",
    "        },\n",
    "        \"LP\": {\n",
    "            \"quality-of-life\": 0,\n",
    "            \"economic-mobility\": 0,\n",
    "            \"health-and-safety\": 0,\n",
    "        },\n",
    "        \"LN\": {\n",
    "            \"quality-of-life\": 0,\n",
    "            \"economic-mobility\": 0,\n",
    "            \"health-and-safety\": 0,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for category, items in categories.items():\n",
    "        for item in items:\n",
    "            try:\n",
    "                distances = list(results_by_city[city][category][item][\"avg_dist\"])\n",
    "            except:\n",
    "                continue\n",
    "            q1 = distances[0]\n",
    "            q2 = distances[1]\n",
    "            q3 = distances[2]\n",
    "            \n",
    "            if item in classifications[\"quality-of-life\"][category]:\n",
    "                purpose = \"quality-of-life\"\n",
    "            elif item in classifications[\"economic-mobility\"][category]:\n",
    "                purpose = \"economic-mobility\"\n",
    "            else:\n",
    "                purpose = \"health-and-safety\"\n",
    "            \n",
    "            if q1 > q2 < q3:\n",
    "                class_counts[city][\"CCU\"][purpose] += 1\n",
    "            elif q1 < q2 > q3:\n",
    "                class_counts[city][\"CCD\"][purpose] += 1\n",
    "            elif q1 < q2 < q3:\n",
    "                class_counts[city][\"LP\"][purpose] += 1\n",
    "            elif q1 > q2 > q3:\n",
    "                class_counts[city][\"LN\"][purpose] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mux = pd.MultiIndex.from_product([city_names, [\"CCU\", \"CCD\", \"LP\", \"LN\"], [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]])\n",
    "df = pd.DataFrame(class_counts, columns=mux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = []\n",
    "\n",
    "for city, categories in class_counts.items():\n",
    "    for category, metrics in categories.items():\n",
    "        for metric, value in metrics.items():\n",
    "            flattened_data.append((city, category, metric, value))\n",
    "\n",
    "df = pd.DataFrame(flattened_data, columns=[\"City\", \"Category\", \"Metric\", \"Value\"])\n",
    "\n",
    "df_pivot = df.pivot(index=\"City\", columns=[\"Metric\", \"Category\"], values=\"Value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = {\"CCD\": {}, \"CCU\": {}, \"LP\": {}, \"LN\": {}}\n",
    "\n",
    "for city, categories in class_counts.items():\n",
    "    \n",
    "    city_totals = {}\n",
    "    for category, metrics in categories.items():\n",
    "        category_counts[category][city] = sum(metrics.values())\n",
    "\n",
    "counts_df = pd.DataFrame(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg = df.groupby([\"Metric\", \"Category\"])[\"Value\"].mean().reset_index()\n",
    "\n",
    "metrics = df_avg[\"Metric\"].unique()\n",
    "\n",
    "fig, axs = plt.subplots(1, len(metrics), figsize=(15, 6), sharey=True)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    metric_data = df_avg[df_avg[\"Metric\"] == metric]\n",
    "    axs[i].bar(metric_data[\"Category\"], metric_data[\"Value\"], color='skyblue')\n",
    "    axs[i].set_title(f\"{metric.capitalize()}\")\n",
    "    axs[i].set_xlabel(\"Category\")\n",
    "    axs[i].set_ylabel(\"Average # of trendlines (out of 4)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_founding = {\n",
    "    \"newyork\": 1624,\n",
    "    \"losangeles\": 1781,\n",
    "    \"chicago\": 1803,\n",
    "    \"houston\": 1837,\n",
    "    \"phoenix\": 1868,\n",
    "    \"philadelphia\": 1681,\n",
    "    \"sanantonio\": 1718,\n",
    "    \"sandiego\": 1769,\n",
    "    \"dallas\": 1841,\n",
    "    \"jacksonville\": 1791,\n",
    "    \"austin\": 1835,\n",
    "    \"fortworth\": 1849, \n",
    "    \"sanjose\": 1777,\n",
    "    \"columbus\": 1812,\n",
    "    \"charlotte\": 1755,\n",
    "    \"indianapolis\": 1821,\n",
    "    \"sanfrancisco\": 1776,\n",
    "    \"seattle\": 1851,\n",
    "    \"denver\": 1858,\n",
    "    \"oklahomacity\": 1889,\n",
    "    \"nashville\": 1779,\n",
    "    \"washington\": 1790,\n",
    "    \"elpaso\": 1659,\n",
    "    \"lasvegas\": 1911,\n",
    "    \"boston\": 1630,\n",
    "    \"detroit\": 1701,\n",
    "    \"portland\": 1845,\n",
    "    \"louisville\": 1778,\n",
    "    \"memphis\": 1819,\n",
    "    \"baltimore\": 1729,\n",
    "    \"milwaukee\": 1833,\n",
    "    \"albuquerque\": 1706,\n",
    "    \"tucson\": 1775,\n",
    "    \"fresno\": 1872,\n",
    "    \"sacramento\": 1848,\n",
    "    \"mesa\": 1878, \n",
    "    \"atlanta\": 1843,\n",
    "    \"kansascity\": 1838\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot[\"Founding Date\"] = df_pivot.index.get_level_values(\"City\").map(cities_to_founding)\n",
    "\n",
    "df_sorted = df_pivot.reset_index().sort_values(by=\"Founding Date\").set_index([\"City\"])\n",
    "\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "amenities = [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]\n",
    "\n",
    "trendlines = [\"CCU\", \"CCD\", \"LN\", \"LP\"]\n",
    "\n",
    "colors = {\"CCU\": \"blue\", \"CCD\": \"green\", \"LN\": \"orange\", \"LP\": \"purple\"}\n",
    "\n",
    "for i, amenity in enumerate(amenities):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    test_df = df_sorted.loc[:, df_sorted.columns.get_level_values('Metric').isin([amenity, 'Founding Date'])]\n",
    "    test_df = test_df.loc[:, test_df.columns.get_level_values(\"Category\").isin(trendlines + [''])]\n",
    "\n",
    "    for category in trendlines:\n",
    "        founding_dates = test_df[\"Founding Date\"].values\n",
    "        values = test_df[(amenity, category)].values.flatten()\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(founding_dates, values)\n",
    "\n",
    "        trendline = slope * founding_dates + intercept\n",
    "        \n",
    "        alpha_threshold = 0.05\n",
    "        \n",
    "        line_alpha = 1.0 if p_value < alpha_threshold else 0.3\n",
    "        \n",
    "        ax.plot(founding_dates, trendline, color=colors[category], linestyle='--', label=f\"{category} Trendline\", alpha=line_alpha)\n",
    "        \n",
    "        # Print statistical results\n",
    "        print(f\"{amenity} - {category}:\")\n",
    "        print(f\"  Slope: {slope:.3f}\")\n",
    "        print(f\"  Intercept: {intercept:.3f}\")\n",
    "        print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "        print(f\"  P-value: {p_value:.3e}\")\n",
    "        print(f\"  Std Err: {std_err:.3f}\")\n",
    "    \n",
    "    # Customize the subplot\n",
    "    ax.set_title(f\"{amenity.capitalize()}\")\n",
    "    ax.set_xlabel(\"Founding Date\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"# of Trendlines (Out of 4)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_partisanship = {\n",
    "    \"newyork\": 100,\n",
    "    \"losangeles\": 90,\n",
    "    \"chicago\": 90,\n",
    "    \"houston\": 0,\n",
    "    \"phoenix\": 20,\n",
    "    \"philadelphia\": 70,\n",
    "    \"sanantonio\": 0,\n",
    "    \"sandiego\": 90,\n",
    "    \"dallas\": 0,\n",
    "    \"jacksonville\": 30,\n",
    "    \"austin\": 0,\n",
    "    \"fortworth\": 0,\n",
    "    \"sanjose\": 90,\n",
    "    \"columbus\": 40,\n",
    "    \"charlotte\": 10,\n",
    "    \"indianapolis\": 10,\n",
    "    \"sanfrancisco\": 90,\n",
    "    \"seattle\": 10,\n",
    "    \"denver\": 60,\n",
    "    \"oklahomacity\": 0,\n",
    "    \"nashville\": 20,\n",
    "    \"washington\": 10,\n",
    "    \"elpaso\": 0,\n",
    "    \"lasvegas\": 60,\n",
    "    \"boston\": 10,\n",
    "    \"detroit\": 70,\n",
    "    \"portland\": 10,\n",
    "    \"louisville\": 20,\n",
    "    \"memphis\": 20,\n",
    "    \"baltimore\": 90,\n",
    "    \"milwaukee\": 80,\n",
    "    \"albuquerque\": 80,\n",
    "    \"tucson\": 20,\n",
    "    \"fresno\": 90,\n",
    "    \"sacramento\": 90,\n",
    "    \"mesa\": 20,\n",
    "    \"atlanta\": 20,\n",
    "    \"kansascity\": 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot[\"Partisanship\"] = df_pivot.index.get_level_values(\"City\").map(cities_to_partisanship)\n",
    "\n",
    "df_sorted = df_pivot.reset_index().sort_values(by=\"Partisanship\").set_index([\"City\"])\n",
    "\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "amenities = [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]\n",
    "\n",
    "trendlines = [\"CCU\", \"CCD\", \"LN\", \"LP\"]\n",
    "\n",
    "colors = {\"CCU\": \"blue\", \"CCD\": \"green\", \"LN\": \"orange\", \"LP\": \"purple\"}\n",
    "\n",
    "for i, amenity in enumerate(amenities):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    test_df = df_sorted.loc[:, df_sorted.columns.get_level_values('Metric').isin([amenity, 'Partisanship'])]\n",
    "    test_df = test_df.loc[:, test_df.columns.get_level_values(\"Category\").isin(trendlines + [''])]\n",
    "\n",
    "    for category in trendlines:\n",
    "        populations = test_df[\"Partisanship\"].values\n",
    "        values = test_df[(amenity, category)].values.flatten()\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(populations, values)\n",
    "\n",
    "        trendline = slope * populations + intercept\n",
    "        \n",
    "        alpha_threshold = 0.05\n",
    "        \n",
    "        line_alpha = 1.0 if p_value < alpha_threshold else 0.3\n",
    "        \n",
    "        ax.plot(populations, trendline, color=colors[category], linestyle='--', label=f\"{category} Trendline\", alpha=line_alpha)\n",
    "        \n",
    "        print(f\"{amenity} - {category}:\")\n",
    "        print(f\"  Slope: {slope:.3f}\")\n",
    "        print(f\"  Intercept: {intercept:.3f}\")\n",
    "        print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "        print(f\"  P-value: {p_value:.3e}\")\n",
    "        print(f\"  Std Err: {std_err:.3f}\")\n",
    "    \n",
    "    ax.set_title(f\"{amenity.capitalize()}\")\n",
    "    ax.set_xlabel(\"Percent Presidential Elections with Democrat Majority (Last 10 Elections)\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"# of Trendlines (Out of 4)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_distance = {\n",
    "    \"newyork\": 40.6943,\n",
    "    \"losangeles\": 34.1141,\n",
    "    \"chicago\": 41.8375,\n",
    "    \"houston\": 29.786,\n",
    "    \"phoenix\": 33.5722,\n",
    "    \"philadelphia\": 40.0077,\n",
    "    \"sanantonio\": 29.4632,\n",
    "    \"sandiego\": 33.0094,\n",
    "    \"dallas\": 32.7935,\n",
    "    \"jacksonville\": 30.3322,\n",
    "    \"austin\": 30.3005,\n",
    "    \"fortworth\": 32.7817,\n",
    "    \"sanjose\": 37.3012,\n",
    "    \"columbus\": 39.9862,\n",
    "    \"charlotte\": 35.2083,\n",
    "    \"indianapolis\": 39.7771,\n",
    "    \"sanfrancisco\": 37.7558,\n",
    "    \"seattle\": 47.6211,\n",
    "    \"denver\": 39.762,\n",
    "    \"oklahomacity\": 35.4676,\n",
    "    \"nashville\": 36.1715,\n",
    "    \"washington\": 38.9047,\n",
    "    \"elpaso\": 31.8476,\n",
    "    \"lasvegas\": 36.2333,\n",
    "    \"boston\": 42.3188,\n",
    "    \"detroit\": 42.3834,\n",
    "    \"portland\": 45.5371,\n",
    "    \"louisville\": 38.1663,\n",
    "    \"memphis\": 35.1087,\n",
    "    \"baltimore\": 39.3051,\n",
    "    \"milwaukee\": 43.0642,\n",
    "    \"albuquerque\": 35.1054,\n",
    "    \"tucson\": 32.1541,\n",
    "    \"fresno\": 36.783,\n",
    "    \"sacramento\": 38.5677,\n",
    "    \"mesa\": 33.4015,\n",
    "    \"atlanta\": 33.7628,\n",
    "    \"kansascity\": 39.1238,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot[\"Distance\"] = df_pivot.index.get_level_values(\"City\").map(cities_to_distance)\n",
    "\n",
    "df_sorted = df_pivot.reset_index().sort_values(by=\"Distance\").set_index([\"City\"])\n",
    "\n",
    "df_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "amenities = [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]\n",
    "\n",
    "colors = {\"CCU\": \"blue\", \"CCD\": \"green\", \"LN\": \"orange\", \"LP\": \"purple\"}\n",
    "\n",
    "for i, amenity in enumerate(amenities):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    test_df = df_sorted.loc[:, df_sorted.columns.get_level_values('Metric').isin([amenity, 'Distance'])]\n",
    "    test_df = test_df.loc[:, test_df.columns.get_level_values(\"Category\").isin(trendlines + [''])]\n",
    "    \n",
    "    for category in trendlines:\n",
    "        founding_dates = test_df[\"Distance\"].values\n",
    "        values = test_df[(amenity, category)].values.flatten()\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(founding_dates, values)\n",
    "        \n",
    "        trendline = slope * founding_dates + intercept\n",
    "        \n",
    "        alpha_threshold = 0.05\n",
    "        \n",
    "        line_alpha = 1.0 if p_value < alpha_threshold else 0.3\n",
    "        \n",
    "        ax.plot(founding_dates, trendline, color=colors[category], linestyle='--', label=f\"{category} Trendline\", alpha=line_alpha)\n",
    "        \n",
    "        print(f\"{amenity} - {category}:\")\n",
    "        print(f\"  Slope: {slope:.3f}\")\n",
    "        print(f\"  Intercept: {intercept:.3f}\")\n",
    "        print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "        print(f\"  P-value: {p_value:.3e}\")\n",
    "        print(f\"  Std Err: {std_err:.3f}\")\n",
    "    \n",
    "    ax.set_title(f\"{amenity.capitalize()}\")\n",
    "    ax.set_xlabel(\"Latitude\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"# of Trendlines (Out of 4)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_to_gini = {\n",
    "    \"newyork\": 0.5136,\n",
    "    \"losangeles\": 0.4928,\n",
    "    \"chicago\": 0.4817,\n",
    "    \"houston\": 0.4827,\n",
    "    \"phoenix\": 0.4534,\n",
    "    \"philadelphia\": 0.4828,\n",
    "    \"sanantonio\": 0.4600,\n",
    "    \"sandiego\": 0.4602,\n",
    "    \"dallas\": 0.4637,\n",
    "    \"jacksonville\": 0.4663,\n",
    "    \"austin\": 0.4563,\n",
    "    \"fortworth\": 0.4637,\n",
    "    \"sanjose\": 0.4743,\n",
    "    \"columbus\": 0.4583,\n",
    "    \"charlotte\": 0.4749,\n",
    "    \"indianapolis\": 0.4655,\n",
    "    \"sanfrancisco\": 0.4913,\n",
    "    \"seattle\": 0.4620,\n",
    "    \"denver\": 0.4482,\n",
    "    \"oklahomacity\": 0.4646,\n",
    "    \"nashville\": 0.4646,\n",
    "    \"washington\": 0.4472,\n",
    "    \"elpaso\": 0.4663,\n",
    "    \"lasvegas\": 0.4706,\n",
    "    \"boston\": 0.4847,\n",
    "    \"detroit\": 0.4726,\n",
    "    \"portland\": 0.4491,\n",
    "    \"louisville\": 0.4624,\n",
    "    \"memphis\": 0.4897,\n",
    "    \"baltimore\": 0.4603,\n",
    "    \"milwaukee\": 0.4710,\n",
    "    \"albuquerque\": 0.4668,\n",
    "    \"tucson\": 0.4679,\n",
    "    \"fresno\": 0.4730,\n",
    "    \"sacramento\": 0.4556,\n",
    "    \"mesa\": 0.4564,\n",
    "    \"atlanta\": 0.4676,\n",
    "    \"kansascity\": 0.4518,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot[\"Gini\"] = df_pivot.index.get_level_values(\"City\").map(cities_to_gini)\n",
    "\n",
    "df_sorted = df_pivot.reset_index().sort_values(by=\"Gini\").set_index([\"City\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n",
    "\n",
    "amenities = [\"quality-of-life\", \"economic-mobility\", \"health-and-safety\"]\n",
    "\n",
    "colors = {\"CCU\": \"blue\", \"CCD\": \"green\", \"LN\": \"orange\", \"LP\": \"purple\"}\n",
    "\n",
    "for i, amenity in enumerate(amenities):\n",
    "    ax = axs[i]\n",
    "    \n",
    "    test_df = df_sorted.loc[:, df_sorted.columns.get_level_values('Metric').isin([amenity, 'Gini'])]\n",
    "    test_df = test_df.loc[:, test_df.columns.get_level_values(\"Category\").isin(trendlines + [''])]\n",
    "    \n",
    "    for category in trendlines:\n",
    "        founding_dates = test_df[\"Gini\"].values\n",
    "        values = test_df[(amenity, category)].values.flatten()\n",
    "\n",
    "        slope, intercept, r_value, p_value, std_err = linregress(founding_dates, values)\n",
    "        \n",
    "        trendline = slope * founding_dates + intercept\n",
    "        \n",
    "        alpha_threshold = 0.05\n",
    "        \n",
    "        line_alpha = 1.0 if p_value < alpha_threshold else 0.3\n",
    "        \n",
    "        ax.plot(founding_dates, trendline, color=colors[category], linestyle='--', label=f\"{category} Trendline\", alpha=line_alpha)\n",
    "        \n",
    "        print(f\"{amenity} - {category}:\")\n",
    "        print(f\"  Slope: {slope:.3f}\")\n",
    "        print(f\"  Intercept: {intercept:.3f}\")\n",
    "        print(f\"  R-squared: {r_value**2:.3f}\")\n",
    "        print(f\"  P-value: {p_value:.3e}\")\n",
    "        print(f\"  Std Err: {std_err:.3f}\")\n",
    "    \n",
    "    ax.set_title(f\"{amenity.capitalize()}\")\n",
    "    ax.set_xlabel(\"Gini Index\")\n",
    "    if i == 0:\n",
    "        ax.set_ylabel(\"# of Trendlines (Out of 4)\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-OOD_env]",
   "language": "python",
   "name": "conda-env-.conda-OOD_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
